The goal of this paper is to improve cross-lingual language understanding ( XLU ) , by carefully studying the effects of training unsupervised crosslingual representations at a very large scale . We present XLM-R a transformer-based multilingual masked language model pre-trained on text in 100 languages , which obtains state-of-the-art performance on cross-lingual classification , sequence labeling and question answering . Multilingual masked language models ( MLM ) like mBERT ( Devlin et al. , 2018 ) and XLM ( Lample and have pushed the stateof-the-art on cross-lingual understanding tasks by jointly pretraining large Transformer models ( Vaswani et al. , 2017 ) on many languages . These models allow for effective cross-lingual transfer , as seen in a number of benchmarks including cross-lingual natural language inference ( Bowman et al. , 2015 ; Williams et al. , 2017 ; Conneau et al. , 2018 ) , question answering ( Rajpurkar et al. , 2016 ; , and named entity recognition ( Pires et al. , 2019 ; Wu and Dredze , 2019 ) . However , all of these studies pre-train on Wikipedia , which provides a relatively limited scale especially for lower resource languages . In this paper , we first present a comprehensive analysis of the trade-offs and limitations of multilingual language models at scale , inspired by recent monolingual scaling efforts . We measure the trade-off between high-resource and low-resource languages and the impact of language sampling and vocabulary size . The experiments expose a trade-off as we scale the number of languages for a fixed model capacity : more languages leads to better cross-lingual performance on low-resource languages up until a point , after which the overall performance on monolingual and cross-lingual benchmarks degrades . We refer to this tradeoff as the curse of multilinguality , and show that it can be alleviated by simply increasing model capacity . We argue , however , that this remains an important limitation for future XLU systems which may aim to improve performance with more modest computational budgets . Our best model XLM-RoBERTa ( XLM-R ) outperforms mBERT on cross-lingual classification by up to 23 % accuracy on low-resource languages . It outperforms the previous state of the art by 5.1 % average accuracy on XNLI , 2.42 % average F1-score on Named Entity Recognition , and 9.1 % average F1-score on cross-lingual Question Answering . We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks , where XLM-R obtains results competitive with state-of-the-art monolingual models , including RoBERTa . These results demonstrate , for the first time , that it is possible to have a single large model for all languages , without sacrificing per-language performance . We will make our code , models and data publicly available , with the hope that this will help research in multilingual NLP and low-resource language understanding . From pretrained word embeddings ( Mikolov et al. , 2013b ; Pennington et al. , 2014 ) to pretrained contextualized representations ( Peters et al. , 2018 ; Schuster et al. , 2019 ) and transformer based language models ( Radford et al. , 2018 ; Devlin et al. , 2018 ) , unsupervised representation learning has significantly improved the state of the art in natural language understanding . Parallel work on cross-lingual understanding ( Mikolov et al. , 2013a ; Schuster et al. , 2019 ; Lample and Conneau , 2019 ) extends these systems to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages . Most recently , Devlin et al . ( 2018 ) and Lample and Conneau ( 2019 ) introduced mBERT and XLM -masked language models trained on multiple languages , without any cross-lingual supervision . Lample and Conneau ( 2019 ) propose translation language modeling ( TLM ) as a way to leverage parallel data and obtain a new state of the art on the cross-lingual natural language inference ( XNLI ) benchmark ( Conneau et al. , 2018 ) . They further show strong improvements on unsupervised machine translation and pretraining for sequence generation . shows that monolingual BERT representations are similar across languages , explaining in part the natural emergence of multilinguality in bottleneck architectures . Separately , Pires et al . ( 2019 ) demonstrated the effectiveness of multilingual models like mBERT on sequence labeling tasks . Huang et al . ( 2019 ) showed gains over XLM using cross-lingual multi-task learning , and Singh et al . ( 2019 ) demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI . However , all of this work was at a relatively modest scale , in terms of the amount of training data , as compared to our approach . The benefits of scaling language model pretraining by increasing the size of the model as well as the training data has been extensively studied in the literature . For the monolingual case , Jozefowicz et al . ( 2016 ) show how large-scale LSTM models can obtain much stronger performance on language modeling benchmarks when trained on billions of tokens . GPT ( Radford et al. , 2018 ) also highlights the importance of scaling the amount of data and RoBERTa shows that training BERT longer on more data leads to significant boost in performance . Inspired by RoBERTa , we show that mBERT and XLM are undertuned , and that simple improvements in the learning procedure of unsupervised MLM leads to much better performance . We train on cleaned CommonCrawls ( Wenzek et al. , 2019 ) , which increase the amount of data for low-resource languages by two orders of magnitude on average . Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages ( Grave et al. , 2018 ) . Several efforts have trained massively multilingual machine translation models from large parallel corpora . They uncover the high and low resource trade-off and the problem of capacity dilution ( Johnson et al. , 2017 ; Tan et al. , 2019 ) . The work most similar to ours is , which trains a single model in 103 languages on over 25 billion parallel sentences . Siddhant et al . ( 2019 ) further analyze the representations obtained by the encoder of a massively multilingual machine translation system and show that it obtains similar results to mBERT on cross-lingual NLI . Our work , in contrast , focuses on the unsupervised learning of cross-lingual representations and their transfer to discriminative tasks . In this section , we present the training objective , languages , and data we use . We follow the XLM approach ( Lample and Conneau , 2019 ) as closely as possible , only introducing changes that improve performance at scale . Masked Language Models . We use a Transformer model ( Vaswani et al. , 2017 ) trained with the multilingual MLM objective ( Devlin et al. , 2018 ; Lample and Conneau , 2019 ) using only monolingual data . We sample streams of text from each language and train the model to predict the masked tokens in the input . We apply subword tok- Dataset size ( in GB ) CommonCrawl Wikipedia Figure 1 : Amount of data in GiB ( log-scale ) for the 88 languages that appear in both the Wiki-100 corpus used for mBERT and XLM-100 , and the CC-100 used for XLM-R. CC-100 increases the amount of data by several orders of magnitude , in particular for low-resource languages . enization directly on raw text data using Sentence Piece ( Kudo and Richardson , 2018 ) with a unigram language model ( Kudo , 2018 ) . We sample batches from different languages using the same sampling distribution as Lample and Conneau ( 2019 ) , but with Î± = 0.3 . Unlike Lample and Conneau 2019 , we do not use language embeddings , which allows our model to better deal with code-switching . We use a large vocabulary size of 250K with a full softmax and train two different models : XLM-R Base ( L = 12 , H = 768 , A = 12 , 270M params ) and XLM-R ( L = 24 , H = 1024 , A = 16 , 550M params ) . For all of our ablation studies , we use a BERT Base architecture with a vocabulary of 150K tokens . Appendix B goes into more details about the architecture of the different models referenced in this paper . Scaling to a hundred languages . XLM-R is trained on 100 languages ; we provide a full list of languages and associated statistics in Appendix A . Figure 1 specifies the iso codes of 88 languages that are shared across XLM-R and XLM-100 , the model from Lample and Conneau ( 2019 ) trained on Wikipedia text in 100 languages . Compared to previous work , we replace some languages with more commonly used ones such as romanized Hindi and traditional Chinese . In our ablation studies , we always include the 7 languages for which we have classification and sequence labeling evaluation benchmarks : English , French , German , Russian , Chinese , Swahili and Urdu . We chose this set as it covers a suitable range of language families and includes low-resource languages such as Swahili and Urdu . We also consider larger sets of 15 , 30 , 60 and all 100 languages . When reporting results on high-resource and lowresource , we refer to the average of English and French results , and the average of Swahili and Urdu results respectively . Scaling the Amount of Training Data . Following Wenzek et al . 20192 , we build a clean Com-monCrawl Corpus in 100 languages . We use an internal language identification model in combination with the one from fastText ( Joulin et al. , 2017 ) . We train language models in each language and use it to filter documents as described in Wenzek et al . ( 2019 ) . We consider one CommonCrawl dump for English and twelve dumps for all other languages , which significantly increases dataset sizes , especially for low-resource languages like Burmese and Swahili . Figure 1 shows the difference in size between the Wikipedia Corpus used by mBERT and XLM-100 , and the CommonCrawl Corpus we use . As we show in Section 5.3 , monolingual Wikipedia corpora are too small to enable unsupervised representation learning . Based on our experiments , we found that a few hundred MiB of text data is usually a minimal size for learning a BERT model . We consider four evaluation benchmarks . For crosslingual understanding , we use cross-lingual natural language inference , named entity recognition , and question answering . We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models . Cross-lingual Natural Language Inference ( XNLI ) . The XNLI dataset comes with groundtruth dev and test sets in 15 languages , and a ground-truth English training set . The training set has been machine-translated to the remaining 14 languages , providing synthetic training data for these languages as well . We evaluate our model on cross-lingual transfer from English to other lan-guages . We also consider three machine translation baselines : ( i ) translate-test : dev and test sets are machine-translated to English and a single English model is used ( ii ) translate-train ( per-language ) : the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set ( iii ) translate-train-all ( multi-language ) : we fine-tune a multilingual model on the concatenation of all training sets from translate-train . For the translations , we use the official data provided by the XNLI project . Named Entity Recognition . For NER , we consider the CoNLL-2002 ( Sang , 2002 and CoNLL-2003 ( Tjong Kim Sang and De Meulder , 2003 ) datasets in English , Dutch , Spanish and German . We fine-tune multilingual models either ( 1 ) on the English set to evaluate cross-lingual transfer , ( 2 ) on each set to evaluate per-language performance , or ( 3 ) on all sets to evaluate multilingual learning . We report the F1 score , and compare to baselines from Lample et al . ( 2016 ) and Akbik et al . ( 2018 ) . Cross-lingual Question Answering . We use the MLQA benchmark from , which extends the English SQuAD benchmark to Spanish , German , Arabic , Hindi , Vietnamese and Chinese . We report the F1 score as well as the exact match ( EM ) score for cross-lingual transfer from English . GLUE Benchmark . Finally , we evaluate the English performance of our model on the GLUE benchmark ( Wang et al. , 2018 ) which gathers multiple classification tasks , such as MNLI ( Williams et al. , 2017 ) , SST-2 ( Socher et al. , 2013 ) , or QNLI ( Rajpurkar et al. , 2018 ) . We use BERT Large and RoBERTa as baselines . In this section , we perform a comprehensive analysis of multilingual masked language models . We conduct most of the analysis on XNLI , which we found to be representative of our findings on other tasks . We then present the results of XLM-R on cross-lingual understanding and GLUE . Finally , we compare multilingual and monolingual models , and present results on low-resource languages . Much of the work done on understanding the crosslingual effectiveness of mBERT or XLM ( Pires et al. , 2019 ; Wu and Dredze , 2019 ; has focused on analyzing the performance of fixed pretrained models on downstream tasks . In this section , we present a comprehensive study of different factors that are important to pretraining large scale multilingual models . We highlight the trade-offs and limitations of these models as we scale to one hundred languages . Transfer-dilution Trade-off and Curse of Multilinguality . Model capacity ( i.e . the number of parameters in the model ) is constrained due to practical considerations such as memory and speed during training and inference . For a fixed sized model , the per-language capacity decreases as we increase the number of languages . While low-resource language performance can be improved by adding similar higher-resource languages during pretraining , the overall downstream performance suffers from this capacity dilution . Positive transfer and capacity dilution have to be traded off against each other . We illustrate this trade-off in Figure 2 , which shows XNLI performance vs the number of languages the model is pretrained on . Initially , as we go from 7 to 15 languages , the model is able to take advantage of positive transfer which improves performance , especially on low resource languages . Beyond this point the curse of multilinguality kicks in and degrades performance across all languages . Specifically , the overall XNLI accuracy decreases from 71.8 % to 67.7 % as we go from XLM-7 to XLM-100 . The same trend can be observed for models trained on the larger CommonCrawl Corpus . The issue is even more prominent when the capacity of the model is small . To show this , we pretrain models on Wikipedia Data in 7 , 30 and 100 languages . As we add more languages , we make the Transformer wider by increasing the hidden size from 768 to 960 to 1152 . In Figure 4 , we show that the added capacity allows XLM-30 to be on par with XLM-7 , thus overcoming the curse of multilinguality . The added capacity for XLM-100 , however , is not enough and it still lags behind due to higher vocabulary dilution ( recall from Section 3 that we used a fixed vocabulary size of 150K for all models ) . High-resource vs Low-resource Trade-off . The allocation of the model capacity across languages is controlled by several parameters : the training set size , the size of the shared subword vocabulary , and the rate at which we sample training examples from each language . We study the effect of sampling on the performance of highresource ( English and French ) and low-resource ( Swahili and Urdu ) languages for an XLM-100 model trained on Wikipedia ( we observe a similar trend for the construction of the subword vocab ) . Specifically , we investigate the impact of varying the Î± parameter which controls the exponential smoothing of the language sampling rate . Similar to Lample and Conneau ( 2019 ) , we use a sampling rate proportional to the number of sentences in each corpus . Models trained with higher values of Î± see batches of high-resource languages more often . Figure 5 shows that the higher the value of Î± , the better the performance on high-resource languages , and vice-versa . When considering overall performance , we found 0.3 to be an optimal value for Î± , and use this for XLM-R . Importance of Capacity and Vocabulary . In previous sections and in Figure 4 , we showed the importance of scaling the model size as we increase the number of languages . Similar to the overall model size , we argue that scaling the size of the shared vocabulary ( the vocabulary capacity ) can improve the performance of multilingual models on downstream tasks . To illustrate this effect , we train XLM-100 models on Wikipedia data with different vocabulary sizes . We keep the overall number of parameters constant by adjusting the width of the transformer . Figure 6 shows that even with a fixed capacity , we observe a 2.8 % increase in XNLI average accuracy as we increase the vocabulary size from 32K to 256K . This suggests that multilingual models can benefit from allocating a higher proportion of the total number of parameters to the embedding layer even though this reduces the size of the Transformer . For simplicity and given the softmax computational constraints , we use a vocabulary of 250k for XLM-R . We further illustrate the importance of this parameter , by training three models with the same transformer architecture ( BERT Base ) but with different vocabulary sizes : 128K , 256K and 512K . We observe more than 3 % gains in overall accuracy on XNLI by simply increasing the vocab size from 128k to 512k . Larger-scale Datasets and Training . As shown in Figure 1 , the CommonCrawl Corpus that we collected has significantly more monolingual data than the previously used Wikipedia corpora . Figure 3 shows that for the same BERT Base architecture , all models trained on CommonCrawl obtain significantly better performance . Apart from scaling the training data , also showed the benefits of training MLMs longer . In our experiments , we observed similar effects of large-scale training , such as increasing batch size ( see Figure 7 ) and training time , on model performance . Specifically , we found that using validation perplexity as a stopping criterion for pretraining caused the multilingual MLM in Lample and Conneau ( 2019 ) to be under-tuned . In our experience , performance on downstream tasks continues to improve even after validation perplexity has plateaued . Combining this observation with our implementation of the unsupervised XLM-MLM objective , we were able to improve the performance of Lample and Conneau ( 2019 ) from 71.3 % to more than 75 % average accuracy on XNLI , which was on par with their supervised translation language modeling ( TLM ) objective . Based on these results , and given our focus on unsupervised learning , we decided to not use the supervised TLM objective for training our models . Simplifying Multilingual Tokenization with Sentence Piece . The different language-specific tokenization tools used by mBERT and XLM-100 make these models more difficult to use on raw text . Instead , we train a Sentence Piece model ( SPM ) and apply it directly on raw text data for all languages . We did not observe any loss in performance for models trained with SPM when compared to models trained with language-specific preprocessing and byte-pair encoding ( see Figure 7 ) and hence use SPM for XLM-R . Based on these results , we adapt the setting of Lample and and use a large Transformer model with 24 layers and 1024 hidden states , with a 250k vocabulary . We use the multilingual MLM loss and train our XLM-R model for 1.5 Million updates on five-hundred 32GB Nvidia V100 GPUs with a batch size of 8192 . We leverage the SPM-preprocessed text data from Common-Crawl in 100 languages and sample languages with Î± = 0.3 . In this section , we show that it out-performs all previous techniques on cross-lingual benchmarks while getting performance on par with RoBERTa on the GLUE benchmark . XNLI . Table 1 shows XNLI results and adds some additional details : ( i ) the number of models the approach induces ( # M ) , ( ii ) the data on which the model was trained ( D ) , and ( iii ) the number of languages the model was pretrained on ( # lg ) . As we show in our results , these parameters significantly impact performance . Column # M specifies whether model selection was done separately on the dev set of each language ( N models ) , or on the joint dev set of all the languages ( single model ) . We observe a 0.6 decrease in overall accuracy when we go from N models to a single model -going from 71.3 to 70.7 . We encourage the community to adopt this setting . For cross-lingual transfer , while this approach is not fully zero-shot transfer , we argue that in real applications , a small amount of supervised data is often available for validation in each language . XLM-R sets a new state of the art on XNLI . On cross-lingual transfer , XLM-R obtains 80.9 % accuracy , outperforming the XLM-100 and mBERT open-source models by 10.2 % and 14.6 % average accuracy . On the Swahili and Urdu lowresource languages , XLM-R outperforms XLM-100 by 15.7 % and 11.4 % , and mBERT by 23.5 % and 15.8 % . While XLM-R handles 100 languages , we also show that it outperforms the former state of the art Unicoder ( Huang et al. , 2019 ) and XLM ( MLM+TLM ) , which handle only 15 languages , by 5.5 % and 5.8 % average accuracy respectively . Using the multilingual training of translate-train-all , XLM-R further improves performance and reaches 83.6 % accuracy , a new overall state of the art for XNLI , outperforming Unicoder by 5.1 % . Multilingual training is similar to practical applications where training sets are available in various languages for the same task . In the case of XNLI , datasets have been translated , and translate-trainall can be seen as some form of cross-lingual data augmentation ( Singh et al. , 2019 ) , similar to backtranslation ( Xie et al. , 2019 ) . Named Entity Recognition . In Table 2 , we report results of XLM-R and mBERT on CoNLL-2002 and CoNLL-2003 . We consider the LSTM + CRF approach from Lample et al . ( 2016 ) and the Flair model from Akbik et al . ( 2018 ) as baselines . We evaluate the performance of the model Table 1 : Results on cross-lingual classification . We report the accuracy on each of the 15 XNLI languages and the average accuracy . We specify the dataset D used for pretraining , the number of models # M the approach requires and the number of languages # lg the model handles . Our XLM-R results are averaged over five different seeds . We show that using the translate-train-all approach which leverages training sets from multiple languages , XLM-R obtains a new state of the art on XNLI of 83.6 % average accuracy . Results with â  are from Huang et al . ( 2019 ) . -2002 and CoNLL-2003 ( F1 score ) . Results with â  are from Wu and Dredze ( 2019 ) . Note that mBERT and XLM-R do not use a linear-chain CRF , as opposed to Akbik et al . ( 2018 ) and Lample et al . ( 2016 ) . on each of the target languages in three different settings : ( i ) train on English data only ( en ) ( ii ) train on data in target language ( each ) ( iii ) train on data in all languages ( all ) . Results of mBERT are reported from Wu and Dredze ( 2019 ) . Note that we do not use a linear-chain CRF on top of XLM-R and mBERT representations , which gives an advantage to Akbik et al . ( 2018 ) . Without the CRF , our XLM-R model still performs on par with the state of the art , outperforming Akbik et al . ( 2018 ) on Dutch by 2.09 points . On this task , XLM-R also outperforms mBERT by 2.42 F1 on average for cross-lingual transfer , and 1.86 F1 when trained on each language . Training on all languages leads to an average F1 score of 89.43 % , outperforming cross-lingual transfer approach by 8.49 % . Question Answering . We also obtain new state of the art results on the MLQA cross-lingual question answering benchmark , introduced by . We follow their procedure by training on the English training data and evaluating on the 7 languages of the dataset . We report results in Table 3 . XLM-R obtains F1 and accuracy scores of 70.7 % and 52.7 % while the previous state of the art was 61.6 % and 43.5 % . XLM-R also outperforms mBERT by 13.0 % F1-score and 11.1 % accuracy . It even outperforms BERT-Large on English , confirming its strong monolingual performance . In this section , we present results of multilingual XLM models against monolingual BERT models . GLUE : XLM-R versus RoBERTa . Our goal is to obtain a multilingual model with strong performance on both , cross-lingual understanding tasks as well as natural language understanding tasks for each language . To that end , we evaluate XLM-R on the GLUE benchmark . We show in Table 4 , that XLM-R obtains better average dev performance than BERT Large by 1.6 % and reaches performance on par with XLNet Large . The RoBERTa model outperforms XLM-R by only 1.0 % on average . We believe future work can reduce this gap even further by alleviating the curse of multilinguality and . We compare the performance of XLM-R to BERT Large , XLNet and RoBERTa on the English GLUE benchmark . XNLI : XLM versus BERT . A recurrent criticism against multilingual models is that they obtain worse performance than their monolingual counterparts . In addition to the comparison of XLM-R and RoBERTa , we provide the first comprehensive study to assess this claim on the XNLI benchmark . We extend our comparison between multilingual XLM models and monolingual BERT models on 7 languages and compare performance in Table 5 . We train 14 monolingual BERT models on Wikipedia and CommonCrawl ( capped at 60 GiB ) , and two XLM-7 models . We increase the vocabulary size of the multilingual model for a better comparison . We found that multilingual models can outperform their monolingual BERT counterparts . Specifically , in Table 5 , we show that for cross-lingual transfer , monolingual baselines outperform XLM-7 for both Wikipedia and CC by 1.6 % and 1.3 % average accuracy . However , by making use of multilingual training ( translate-trainall ) and leveraging training sets coming from multiple languages , XLM-7 can outperform the BERT models : our XLM-7 trained on CC obtains 80.0 % average accuracy on the 7 languages , while the average performance of BERT models trained on CC is 77.5 % . This is a surprising result that shows that the capacity of multilingual models to leverage training data coming from multiple languages for a particular task can overcome the capacity dilution problem to obtain better overall performance . 87.2 82.5 82.9 79.7 80.4 75.7 71.5 80.0 Table 5 : Multilingual versus monolingual models ( BERT-BASE ) . We compare the performance of monolingual models ( BERT ) versus multilingual models ( XLM ) on seven languages , using a BERT-BASE architecture . We choose a vocabulary size of 40k and 150k for monolingual and multilingual models . We observed in Table 5 that pretraining on Wikipedia for Swahili and Urdu performed similarly to a randomly initialized model ; most likely due to the small size of the data for these languages . On the other hand , pretraining on CC improved performance by up to 10 points . This confirms our assumption that mBERT and XLM-100 rely heavily on cross-lingual transfer but do not model the low-resource languages as well as XLM-R . Specifically , in the translate-train-all setting , we observe that the biggest gains for XLM models trained on CC , compared to their Wikipedia counterparts , are on low-resource languages ; 7 % and 4.8 % improvement on Swahili and Urdu respectively . In this work , we introduced XLM-R , our new state of the art multilingual masked language model trained on 2.5 TB of newly created clean Com-monCrawl data in 100 languages . We show that it provides strong gains over previous multilingual models like mBERT and XLM on classification , sequence labeling and question answering . We exposed the limitations of multilingual MLMs , in particular by uncovering the high-resource versus low-resource trade-off , the curse of multilinguality and the importance of key hyperparameters . We also expose the surprising effectiveness of multilingual models over monolingual models , and show strong improvements on low-resource languages . https : //github.com/facebookresearch/cc net 
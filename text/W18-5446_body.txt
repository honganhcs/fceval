Human ability to understand language is general , flexible , and robust . In contrast , most NLU models above the word level are designed for a specific task and struggle with out-of-domain data . If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs , then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains . To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE , gluebenchmark.com ) : a benchmark of nine diverse NLU tasks , an auxiliary dataset for probing models for understanding of specific linguistic phenomena , and an online platform for evaluating and comparing models . For some benchmark tasks , training data is plentiful , but for others it is limited or does not match the genre of the test set . GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks . While none of the datasets in GLUE were created from scratch for the benchmark , four of them feature privately-held test data , which is used to ensure that the benchmark is used fairly . We evaluate baselines that use ELMo ( Peters et al. , 2018 ) , a powerful transfer learning technique , as well as state-of-the-art sentence representation models . The best models still achieve fairly low absolute scores . Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested , with some exceptions . The GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains , data quantities , and difficulties . As the goal of GLUE is to spur development of generalizable NLU systems , we design the benchmark such that good performance should re- quire models to share substantial knowledge ( e.g. , trained parameters ) across tasks , while maintaining some task-specific components . Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark , we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive . , Bar Haim et al . 2006 , Giampiccolo et al . 2007 , Bentivogli et al . 2009 , as well as versions of SQuAD ( Rajpurkar et al. , 2016 ) and Winograd Schema Challenge ( Levesque et al. , 2011 ) recast as NLI ( resp . QNLI , WNLI ) . Table 1 summarizes the tasks . Performance on the benchmark is measured per task as well as in aggregate , averaging performance across tasks . Diagnostic Dataset To understand the types of knowledge learned by models , GLUE also includes a dataset of hand-crafted examples for probing trained models . This dataset is designed to highlight common phenomena , such as the use of world knowledge , logical operators , and lexical entailments , that models must grasp if they are to robustly solve the tasks . Each of the 550 examples is an NLI sentence pair tagged with the phenomena demonstrated . We ensure that the data is reasonably diverse by producing examples for a wide variety of linguistic phenomena , and basing our examples on naturally-occurring sentences from several domains . We validate our data by using the hypothesis-only baseline from Gururangan et al . ( 2018 ) and having six NLP researchers manually validate a random sample of the data . Baselines To demonstrate the benchmark in use , we apply multi-task learning on the training data of the GLUE tasks , via a model that shares a BiL-STM between task-specific classifiers . We also train models that use the same architecture but are trained on a single benchmark task . Finally , we evaluate the following pretrained models : average bag-of-words using GloVe embeddings ( CBoW ) , Skip-Thought ( Kiros et al. , 2015 ) , InferSent ( Conneau et al. , 2017 ) , DisSent ( Nie et al. , 2017 ) , and GenSen ( Subramanian et al. , 2018 ) . I have never seen a hummingbird not flying . I have never seen a hummingbird . Cape sparrows eat seeds , along with soft plant parts and insects . Cape sparrows are eaten . Musk decided to offer up his personal Tesla roadster . Musk decided to offer up his personal car . Table 3 : Diagnostic set examples . Systems must predict the relationship between the sentences , either entailment , neutral , or contradiction when one sentence is the premise and the other is the hypothesis , and vice versa . Examples are tagged with the phenomena demonstrated . We group each phenomena into one of four broad categories . We find that our models trained directly on the GLUE tasks generally outperform those that do not , though all models obtain fairy low absolute scores . Probing the baselines with the diagnostic data , we find that performance on the benchmark correlates with performance on the diagnostic data , and that the best baselines similarly achieve low absolute performance on the linguistic phenomena included in the diagnostic data . We present the GLUE benchmark , consisting of : ( i ) a suite of nine NLU tasks , built on established annotated datasets and covering a diverse range of text genres , dataset sizes , and difficulties ; ( ii ) an online evaluation platform and leaderboard , based primarily on private test data ; ( iii ) an expert-constructed analysis dataset . Experiments indicate that solving GLUE is beyond the capability of current transfer learning methods . data.quora.com/First-Quora-Dataset-Release-Question-Pairs 
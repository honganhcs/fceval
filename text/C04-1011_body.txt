Among the many formalisms used for description and analysis of syntactic structure of natural language , the class of context-free grammars ( CFGs ) is by far the best understood and most widely used . Many formalisms with greater generative power , in particular the different types of unification grammars , are ultimately based on CFGs . Regular expressions , with their procedural counter-part of finite automata ( FAs ) , are not able to describe hierarchical , tree-shaped structure , and thereby seem less suitable than CFGs for full analysis of syntactic structure . However , there are many applications where only partial or approximated analysis of structure is needed , and where full context-free processing could be prohibitively expensive . Such applications can for example be found in real-time speech recognition systems : of the many hypotheses returned by a speech recognizer , shallow syntactic analysis may be used to select a small subset of those that seem most promising for full syntactic processing in a next phase , thereby avoiding further computational costs for the less promising hypotheses . As FAs can not describe structure as such , it is impractical to write the automata required for such applications by hand , and even difficult to derive them automatically by training . For this reason , the used FAs are often derived from CFGs , by means of some form of approximation . An overview of different methods of approximating CFGs by FAs , along with an experimental comparison , was given by ( Nederhof , 2000 ) . The next step is to assign probabilities to the transitions of the approximating FA , as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership . Under certain circumstances , this may be done by carrying over the probabilities from an input probabilistic CFG ( PCFG ) , as shown for the special case of n-grams by ( Rimon and Herz , 1991 ; , or by training of the FA on a corpus generated by the PCFG ( Jurafsky et al. , 1994 ) . See also ( Mohri and Nederhof , 2001 ) for discussion of related ideas . An obvious question to ask is then how well the resulting PFA approximates the input PCFG , possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions . Until now , any direct way of measuring the distance between a PCFG and a PFA has been lacking . As we will argue in this paper , the natural distance measure between probability distributions , the Kullback-Leibler ( KL ) distance , is difficult to compute . ( The KL distance is also called relative entropy . ) We can however derive a closedform ( analytical ) solution for the cross entropy of a PCFG and a PFA , provided the FA underlying the PFA is deterministic . The difference between the cross-entropy and the KL distance is the entropy of the PCFG , which does not rely on the PFA . This means that if we are interested in the relative quality of different approximating PFAs with respect to a single input PCFG , the cross-entropy may be used instead of the KL distance . The constraint of determinism is not a problem in practice , as any FA can be determinized , and FAs derived by approxima-tion algorithms are normally determinized ( and minimized ) . As a second possible application , we now look more closely into the matter of determinization of finite-state models . Not all PFAs can be determinized , as discussed by ( Mohri , 1997 ) . This is unfortunate , as deterministic ( P ) FAs process input with time and space costs independent of the size of the automaton , whereas these costs are linear in the size of the automaton in the nondeterministic case , which may be too high for some real-time applications . Instead of distribution-preserving determinization , we may therefore approximate a nondeterministic PFA by a deterministic PFA whose probability distribution is close to , but not necessarily identical to , that of the first PFA . Again , an important question is how close the two models are to each other . It was argued before by ( Juang and Rabiner , 1985 ; Falkhausen et al. , 1995 ; Vihola et al. , 2002 ) that the KL distance between finitestate models is difficult to compute in general . The theory developed in this paper shows however that the cross-entropy between the input PFA and the approximating deterministic PFA can be expressed in closed form , relying on the fact that a PFA can be seen as a special case of a PCFG . Thereby , different approximating deterministic PFAs can be compared for closeness to the input PFA . We can even compute the KL distance between two unambiguous PFAs , in closed form . ( It is not difficult to see that ambiguity is a decidable property for FAs . ) The structure of this paper is as follows . We provide some preliminary definitions in Section 2 . Section 3 discusses the expected frequency of a rule in derivations allowed by a PCFG , and explains how such values can be effectively computed . The KL distance between a PCFG and a PFA is closely related to the entropy of the PCFG , which we discuss in Section 4 . Essential to our approach is the intersection of PCFGs and PFAs , to be discussed in Section 5 . As we show in Section 6 , the part of the KL distance expressing the cross-entropy can be computed in closed form , based on this intersection . Section 7 concludes this paper . Throughout the paper we use mostly standard formal language notation , as for instance in ( Hopcroft and Ullman , 1979 ; Booth and Thompson , 1973 ) , which we summarize below . A context-free grammar ( CFG ) is a 4-tuple G = ( Σ , N , S , R ) where Σ and N are finite disjoint sets of terminals and nonterminals , respectively , S ∈ N is the start symbol and R is a finite set of rules . Each rule has the form A → α , where A ∈ N and α ∈ ( Σ ∪ N ) * . The 'derives ' relation ⇒ associated with G is defined on triples consisting of two strings α , β ∈ ( Σ ∪ N ) * and a rule π ∈ R. We write α π ⇒ β if and only if α is of the form uAδ and β is of the form uγδ , for some u ∈ Σ * , δ ∈ ( Σ ∪ N ) * , and π = ( A → γ ) . A left-most derivation ( for G ) is a string d = π 1 • • • π m , m ≥ 0 , such that α 0 π 1 ⇒ α 1 π 2 ⇒ • • • πm ⇒ α m , for some α 0 , . . . , α m ∈ ( Σ ∪ N ) * ; d = ( where denotes the empty string ) is also a left-most derivation . In the remainder of this paper , we will let the term 'derivation ' refer to 'leftmost derivation ' , unless specified otherwise . If α 0 π 1 ⇒ • • • πm ⇒ α m for some α 0 , . . . , α m ∈ ( Σ∪N ) * , then we say that d = π 1 • • • π m derives α m from α 0 and we write α 0 d ⇒ α m ; d = derives any α 0 ∈ ( Σ ∪ N ) * from itself . A ( left-most ) derivation d such that S d ⇒ w , w ∈ Σ * , is called a complete derivation . If d is a complete derivation , we write y ( d ) to denote the ( unique ) string w ∈ Σ * such that S d ⇒ w . The language generated by G is the set of all strings y ( d ) derived by complete derivations , i.e. , L ( G ) = { w | S d ⇒ w , d ∈ R * , w ∈ Σ * } . It is well-known that there is a one-to-one correspondence between complete derivations and parse trees for strings in L ( G ) . A probabilistic CFG ( PCFG ) is a pair G p = ( G , p G ) , where G is a CFG and p G is a function from R to real numbers in the interval [ 0 , 1 ] . A PCFG is proper if π= ( A→α ) p G ( π ) = 1 for all A ∈ N . Function p G can be used to associate probabilities to derivations of the underlying CFG G , in the following way . For d = π 1 • • • π m ∈ R * , m ≥ 0 , we define p G ( d ) = m i=1 p G ( π i ) if S d ⇒ w for some w ∈ Σ * , and p G ( d ) = 0 otherwise . The probability of a string w ∈ Σ * is defined as p G ( w ) = d : y ( d ) =w p G ( d ) . A PCFG is consistent if w p G ( w ) = 1 . Consistency implies that the PCFG defines a probability distribution on the set of terminal strings as well as on the set of grammar derivations . If a PCFG is proper , then consistency means that no probability mass is lost in 'infinite ' derivations . A finite automaton ( FA ) is a 5-tuple M = ( Σ , Q , q 0 , Q f , T ) , where Σ and Q are two finite sets of terminals and states , respectively , q 0 is the initial state , Q f ⊆ Q is the set of final states , and T is a finite set of transitions , each of the form s a → t , where s , t ∈ Q and a ∈ Σ . A probabilistic finite automaton ( PFA ) is a pair M p = ( M , p M ) , where M is an FA and p M is a function from T to real numbers in the interval [ 0 , 1 ] . 1 For a fixed ( P ) FA M , we define a configuration to be an element of Q × Σ * , and we define the relation on triples consisting of two configurations and a transition τ ∈ T by : ( s , w ) τ ( t , w ) if and only if w is of the form aw , for some a ∈ Σ , and τ = ( s a → t ) . A complete computation is a string c = τ 1 • • • τ m , m ≥ 0 , such that ( s 0 , w 0 ) τ 1 ( s 1 , w 1 ) τ 2 • • • τm ( s m , w m ) , for some ( s 0 , w 0 ) , . . . , ( s m , w m ) ∈ Q × Σ * , with s 0 = q 0 , s m ∈ Q f and w m = , and we write ( s 0 , w 0 ) c ( s m , w m ) . The language accepted by M is L ( M ) = { w ∈ Σ * | ( q  , w ) c ( s , ) , c ∈ T * , s ∈ Q f } . For a PFA M p = ( M , p M ) , and c = τ 1 • • • τ m ∈ T * , m ≥ 0 , we define p M ( c ) = m i=1 p M ( τ i ) if c is a complete computation , and p M ( c ) = 0 otherwise . A PFA is consistent if c p M ( c ) = 1 . We say M is unambiguous if for each w ∈ Σ * , ∃ s∈Q f [ ( q 0 , w ) c ( s , ) ] for at most one c ∈ T * . We say M is deterministic if for each s and a , there is at most one transition s a → t. Determinism implies unambiguity . It can be more readily checked whether an FA is deterministic than whether it is unambiguous . Furthermore , any FA can be effectively turned into a deterministic FA accepting the same language . Therefore , this paper will assume that FAs are deterministic , although technically , unambiguity is sufficient for our constructions to apply . Here we discuss how we can compute the expectation of the frequency of a rule or a nonterminal over all derivations of a probabilistic context-free grammar . These quantities will be used later by our algorithms . 1 Our definition of PFAs amounts to a slight loss of generality with respect to standard definitions , in that there are no epsilon transitions and no probability function on states being final . We want to avoid these concepts as they would cause some technical complications later in this article . There is no loss of generality however if we may assume an end-of-sentence marker , which is often the case in practice . Let ( A → α ) ∈ R be a rule of PCFG G p , and let d ∈ R * be a complete derivation in G p . We define f ( A → α ; d ) as the number of occurrences , or frequency , of A → α in d. Similarly , the frequency of nonterminal A in d is defined as f ( A ; d ) = α f ( A → α ; d ) . We consider the following related quantities E p G f ( A → α ; d ) = d p G ( d ) • f ( A → α ; d ) , E p G f ( A ; d ) = d p G ( d ) • f ( A ; d ) = α E p G f ( A → α ; d ) . A method for the computation of these quantities is reported in ( Hutchins , 1972 ) , based on the so-called momentum matrix . We propose an alternative method here , based on an idea related to the inside-outside algorithm ( Baker , 1979 ; Lari and Young , 1990 ; Lari and Young , 1991 ) . We observe that we can factorize a derivation d at each occurrence of rule A → α into an 'innermost ' part d 2 and two 'outermost ' parts d 1 and d 3 . We can then write E p G f ( A → α ; d ) = d=π 1 •••πm , m 1 , m 2 , w , β , v , x : S d 1 ⇒wAβ , with d 1 =π 1 •••π m 1 −1 , ( A→α ) =πm 1 , α d 2 ⇒v , with d 2 =π m 1 +1 •••πm 2 , β d 3 ⇒x , with d 3 =π m 2 +1 •••πm m i=1 p G ( π i ) . Next we group together all of the innermost and all of the outermost derivations and write E p G f ( A → α ; d ) = out Gp ( A ) • p G ( A → α ) • in Gp ( α ) where out Gp ( A ) = d=π 1 •••πm , d =π 1 •••π m , w , β , x : S d ⇒wAβ , β d ⇒x m i=1 p G ( π i ) • m i=1 p G ( π i ) and in Gp ( α ) = d=π 1 •••πm , v : α d ⇒v m i=1 p G ( π i ) . Both out Gp ( A ) and in Gp ( α ) can be described in terms of recursive equations , of which the least fixed-points are the required values . If G p is proper and consistent , then in Gp ( α ) = 1 for each α ∈ ( Σ ∪ N ) * . Quantities out Gp ( A ) for every A can all be ( exactly ) calculated by solving a linear system , requiring an amount of time proportional to the cube of the size of G p ; see for instance ( Corazza et al. , 1991 ) . On the basis of all the above quantities , a number of useful statistical properties of G p can be easily computed , such as the expected length of derivations , denoted EDL ( G p ) and the expected length of sentences , denoted EWL ( G p ) , discussed before by ( Wetherell , 1980 ) . These quantities satisfy the relations EDL ( G p ) = E p G |d| = A→α out Gp ( A ) • p G ( A → α ) • in Gp ( α ) , EWL ( G p ) = E p G |y ( d ) | = A→α out Gp ( A ) • p G ( A → α ) • in Gp ( α ) • |α| Σ , where for a string γ ∈ ( N ∪ Σ ) * we write |γ| Σ to denote the number of occurrences of terminal symbols in γ . In this section we introduce the notion of derivational entropy of a PCFG , and discuss an algorithm for its computation . Let G p = ( G , p G ) be a PCFG . For a nonterminal A of G , let us define the entropy of A as the entropy of the distribution p G on all rules of the form A → α , i.e. , H ( A ) = E p G log 1 p G ( A → α ) = α p G ( A → α ) • log 1 p G ( A → α ) . The derivational entropy of G p is defined as the expectation of the information of the complete derivations generated by G p , i.e. , H d ( G p ) = E p G log 1 p G ( d ) = d p G ( d ) • log 1 p G ( d ) . We now characterize derivational entropy using expected rule frequencies as H d ( G p ) = d p G ( d ) • log 1 p G ( d ) = d p G ( d ) • log A→α 1 p G ( A → α ) f ( A→α ; d ) = d p G ( d ) • A→α f ( A → α ; d ) • log 1 p G ( A → α ) = A→α log 1 p G ( A → α ) • d p G ( d ) • f ( A → α ; d ) = A→α log 1 p G ( A → α ) • E p G f ( A → α ; d ) = A α log 1 p G ( A → α ) • out Gp ( A ) • p G ( A → α ) • in Gp ( α ) = A out Gp ( A ) • α p G ( A → α ) • log 1 p G ( A → α ) • in Gp ( α ) . As already discussed , under the assumption that G p is proper and consistent we have in Gp ( α ) = 1 for every α . Thus we can write EQUATION The computation of out Gp ( A ) was discussed in Section 3 , and also H ( A ) can easily be calculated . Under the restrictive assumption that a PCFG is proper and consistent , the characterization in ( 2 ) was already known from ( Grenander , 1976 , Theorem 10.7 , pp . 90-92 ) . The proof reported in that work is different from ours and uses a momentum matrix ( Section 3 ) . Our characterization above is more general and uses simpler notation than the one in ( Grenander , 1976 ) . The sentential entropy , or entropy for short , of G p is defined as the expectation of the information of the strings generated by G p , i.e. , EQUATION assuming 0 • log 1 0 = 0 , for strings w not generated by G p . It is not difficult to see that H ( G p ) ≤ H d ( G p ) and equality holds if and only if G is unambiguous ( Soule , 1974 , Theorem 2.2 ) . As ambiguity of CFGs is undecidable , it follows that we can not hope to obtain a closed-form solution for H ( G p ) for which equality to ( 2 ) is decidable . We will return to this issue in Section 6 . In order to compute the cross-entropy defined in the next section , we need to derive a single probabilistic model that simultaneously accounts for both the computations of an underlying FA and the derivations of an underlying PCFG . We start from a construction originally presented in ( Bar-Hillel et al. , 1964 ) , that computes the intersection of a context-free language and a regular language . The input consists of a CFG G = ( Σ , N , S , R ) and an FA M = ( Σ , Q , q 0 , Q f , T ) ; note that we assume , without loss of generality , that G and M share the same set of terminals Σ . The output of the construction is CFG G ∩ = ( Σ , N ∩ , S ∩ , R ∩ ) , where N ∩ = Q × ( Σ ∪ N ) × Q ∪ { S ∩ } , and R ∩ consists of the set of rules that is obtained as follows . • For each s ∈ Q f , let S ∩ → ( q 0 , S , s ) be a rule of G ∩ . • For each rule A → X 1 • • • X m of G and each sequence of states s 0 , . . . , s m of M , with m ≥ 0 , let ( s 0 , A , s m ) → ( s 0 , X 1 , s 1 ) • • • ( s m−1 , X m , s m ) be a rule of G ∩ ; for m = 0 , G ∩ has a rule ( s 0 , A , s 0 ) → for each state s 0 . • For each transition s a → t of M , let ( s , a , t ) → a be a rule of G ∩ . ( s 0 , A , s m ) → ( s 0 , X 1 , s 1 ) • • • ( s m−1 , X m , s m ) there is a unique rule A → X 1 • • • X m from which it has been constructed by the above . Similarly , each rule ( s , a , t ) → a uniquely identifies a transition s a → t. This means that if we take a complete derivation d ∩ in G ∩ , we can extract a sequence h 1 ( d ∩ ) of rules from G and a sequence h 2 ( d ∩ ) of transitions from M , where h 1 and h 2 are string homomorphisms that we define point-wise as As noted before by ( Nederhof and Satta , 2003 ) , this construction can be extended to apply to a PCFG G p = ( G , p G ) and an FA M . The output is a PCFG G ∩ , p = ( G ∩ , p G∩ ) , where G ∩ is defined as above and p G∩ is defined by : • h 1 ( π ∩ ) = , if π ∩ is S ∩ → ( q 0 , S , s ) ; h 1 ( π ∩ ) = π , if π ∩ is ( s 0 , A , s m ) → ( s 0 , X 1 , s 1 ) • • • ( s m−1 , X m , s m ) and π is ( A → X 1 • • • X m ) ; h 1 ( π ∩ ) = , if π ∩ is ( s , a , t ) → a ; • h 2 ( π ∩ ) = , if π ∩ is S ∩ → ( q 0 , S , s ) ; h 2 ( π ∩ ) = τ , if π ∩ is ( s , a , t ) → a and τ is s a → t ; h 2 ( π ∩ ) = , if π ∩ is ( s 0 , A , s m ) → ( s 0 , X 1 , s 1 ) • • • ( s m−1 , X m , s m ) . We define h ( d ∩ ) = ( h 1 ( d ∩ ) , h 2 ( d ∩ ) ) . It • p G∩ ( S ∩ → ( q 0 , S , s ) ) = 1 ; • p G∩ ( ( s 0 , A , s m ) → ( s 0 , X 1 , s 1 ) • • • ( s m−1 , X m , s m ) ) = p G ( A → X 1 • • • X m ) ; • p G∩ ( ( s , a , t ) → a ) = 1 . Note that G ∩ , p is non-proper . More specifically , probabilities of rules with left-hand side S ∩ or ( s 0 , A , s m ) might not sum to one . This is not a problem for the algorithms presented in this paper , as we have never assumed properness for our PCFGs . What is most important here is the following property of G ∩ , p . If d ∩ , d and c are such that h ( d ∩ ) = ( d , c ) , then p G∩ ( d ∩ ) = p G ( d ) . Let us now assume that M is deterministic . ( In fact , the weaker condition of M being unambiguous is sufficient for our purposes , but unambiguity is not a very practical condition . ) Given a string w and a transition s EQUATION 6 Kullback-Leibler distance In this section we consider the Kullback-Leibler distance between a PCFGs and a PFA , and present a method for its optimization under certain assumptions . Let G p = ( G , p G ) be a consistent PCFG and let M p = ( M , p M ) be a consistent PFA . We demand that M be deterministic ( or more generally , unambiguous ) . Let us first assume that L ( G ) ⊆ L ( M ) ; we will later drop this constraint . The cross-entropy of G p and M p is defined as usual for probabilistic models , viz . as the expectation under distribution p G of the information of the strings generated by M , i.e. , H ( G p || M p ) = E p G log 1 p M ( w ) = w p G ( w ) • log 1 p M ( w ) . The Kullback-Leibler distance of G p and M p is defined as D ( G p || M p ) = E p G log p G ( w ) p M ( w ) = w p G ( w ) • log p G ( w ) p M ( w ) . Quantity D ( G p || M p ) can also be expressed as the difference between the cross-entropy of G p and M p and the entropy of G p , i.e. , D ( G p || M p ) = H ( G p || M p ) − H ( G p ) . ( 5 ) Let G ∩ , p be the PCFG obtained by intersecting G p with the non-probabilistic FA M underlying M p , as in Section 5 . Using ( 4 ) the cross-entropy of G p and M p can be expressed as a , t ) ) . H ( G p || M p ) = w p G ( w ) • log 1 p M ( w ) = d p G ( d ) • log 1 p M ( y ( d ) ) = d p G ( d ) • log s a →t 1 p M ( s a → t ) f ( s a →t ; y ( d ) ) = d p G ( d ) • s a →t f ( s a → t ; y ( d ) ) • log 1 p M ( s a → t ) = s a →t log 1 p M ( s a → t ) • d p G ( d ) • f ( s a → t ; y ( d ) ) = s a →t log 1 p M ( s a → t ) • E p G f ( s a → t ; y ( d ) ) = s a →t log 1 p M ( s a → t ) • out G∩ , p ( ( s , We can combine the above with ( 5 ) to obtain D ( G p || M p ) = s a →t out G∩ , p ( ( s , a , t ) ) • log 1 p M ( s a → t ) − H ( G p ) . The values of out G∩ , p can be calculated easily , as discussed in Section 3 . Computation of H ( G p ) in closed-form is problematic , as already pointed out in Section 4 . However , for many purposes computation of H ( G p ) is not needed . For example , assume that the nonprobabilistic FA M underlying M p is given , and our goal is to measure the distance between G p and M p , for different choices of p M . Then the choice that minimizes H ( G p || M p ) determines the choice that minimizes D ( G p || M p ) , irrespective of H ( G p ) . Formally , we can use the above characterization to compute p * M = argmax p M D ( G p || M p ) = argmax p M H ( G p || M p ) . When L ( G ) − L ( M ) We have discussed the computation of the KL distance between PCFGs and deterministic PFAs . We have argued that exact computation is difficult in general , but for determining the relative qualities of different PFAs , with respect to their closeness to an input PCFG , it suffices to compute the cross-entropy . We have shown that the cross-entropy between a PCFG and a deterministic PFA can be computed exactly . These results can also be used for comparing a pair of PFAs , one of which is deterministic . Generalization of PCFGs to probabilistic treeadjoining grammars ( PTAGs ) is also possible , by means of the intersection of a PTAG and a PFA , along the lines of ( Lang , 1994 ) . 
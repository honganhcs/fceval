This paper presents a new approach to automatic language learning that is based on natural language processing techniques.,This paper presents a new approach for automatic language learning.,This paper presents a new approach for automatic language learning.,"The approach is based on a modular architecture that connects modules for morphological analysis and disambiguation, dictionary access, and (indexed) corpora search with an output module.","The approach is based on a combination of natural language processing techniques, especially morphological processing and corpora analysis, to the technology for intelligent computerassisted language learning.",The proposed approach is based on a combination of morphological processing and corpora analysis.,"The core modules provide the information noted in Section 1, 1-3, and provide the information provided by the user.","The key idea is to use a dictionary to extract morphological information, and then use it to generate a dictionary for reading.",The main idea is to use a dictionary to extract morphological information from a bilingual English/Bulgarian dictionary.,The demonstrators run on both UNIX and Windows '95.,The approach is evaluated on both UNIX and Windows 95.,"Then, the dictionary is searched using a supervised learning algorithm.","The demonstrators have been tested by students, but they might also be put to use to support reading directly by people who are not engaged in formal language instruction.",The results show that the proposed approach is able to support reading of essentially all non-specialized texts.,The experiments show that the proposed approach is able to support reading of essentially all non-specialized texts.,We describe a method for automatically removing the tedium of dictionary use from intermediate language learning.,The approach is tested on both UNIx and Windows 95.,"We describe morphological analysis, bilingual dictionary entry, and an audible pronunciation module.",We present a new approach to automatic morphological analysis for intermediate language learners.,This paper presents a new approach for automatic linguistic processing for intermediate language learners.,This paper presents a new approach for automatic language learning.,"The approach is based on a combination of a morphological analysis module, a dictionary access module, and a disambiguated corpus search module.","The approach is based on a morphological analysis of the word, a dictionary entry, and a disambiguation of the word.",The proposed approach is based on a combination of morphological processing and corpora analysis.,"The core modules provide the information noted in Section 1, 1-3, and provide the information provided by the user.",The approach is evaluated on both UNIX and Windows 95.,The main idea is to use a dictionary to extract morphological information from a bilingual English/Bulgarian dictionary.,The resulting demonstrators run on both UNIX and Windows '95.,The results show that the proposed approach is able to support reading of essentially all non-specialized texts.,"Then, the dictionary is searched using a supervised learning algorithm.","The demonstrators run on a variety of languages, including English, Spanish, French, English, Hungarian, French, and German.",The approach is tested on both UNIx and Windows 95.,The experiments show that the proposed approach is able to support reading of essentially all non-specialized texts.,"The demonstrators have been tested by students, but they might also be put to use to support reading directly by people who are not engaged in formal language instruction.",We describe a method for disambiguating a word in a bilingual English dictionary.
We present a corpus-based investigation of sluices in the British National Corpus (BNC).,We present a corpus-based investigation of sluices in the British National Corpus (BNC).,This paper studies the problem of sluice classification in dialogue.,We use machine learning techniques to extract rules for classifying sluices into dialogue types and discuss the methodology we used in this study.,We use machine learning techniques to extract rules for classifying sluices into dialogue types and discuss the methodology we used in this study.,The authors use machine learning techniques to classify sluices into dialogue types.,We use heuristic principles for classifying each sluice dialogue type and formulate these principles as probability weighted Horn clauses.,"We use heuristic principles to classify sluices, and use them to classify a sluice into dialogue types.",The authors use a heuristic to classify a sluice into a dialogue type.,We use two machine learning algorithms to classify sluices.,We use a memory-based learning algorithm to classify arbitrary sluices.,The authors use two machine learning algorithms to classify arbitrary sluices.,"We use SLIPPER to classify a sluice into a dialogue type, and TiMBL to classify based on a memory-based learning procedure.",We use SLIPPER to classify based sluices and use it to classify lexical sluices from the BNC.,"The first one uses SLIPPER to classify gaussian sluices, and the second one uses a memory-based machine learning procedure to classify bounded sluices from a training set.","We use a rule-based learning algorithm to classify arbitrary sluices, and use two machine learning methods to classify if they resemble our Horn clause principles.",The results show that the two algorithms perform well.,We present a corpus-based investigation of sluices in the British National Corpus (BNC).,We present a corpus-based investigation of sluices in the British National Corpus (BNC).,This paper studies the problem of sluice classification in dialogue.,We use machine learning techniques to extract rules for sluices into dialogue types and discuss the methodology we use in this study.,We use machine learning techniques to extract rules for classifying sluices into dialogue types and discuss the methodology we used in this study.,The authors use machine learning techniques to classify sluices into dialogue types.,"We use heuristic principles to classify sluices, and use them to classify a sluice into dialogue types.","We use heuristic principles to classify sluices, and use them to classify a sluice into dialogue types.",The authors use a heuristic to classify a sluice into a dialogue type.,We use a heuristic algorithm to classify arbitrary sluices.,We use a memory-based machine learning procedure to classify arbitrary sluices.,The authors use two machine learning algorithms to classify arbitrary sluices.,We use SLIPPER to classify recursive sluices and use it to classify lexical sluices from the BNC.,We use SLIPPER to classify recursive sluices and use a memory based machine learning procedure for identifying sluices that closely resemble our Horn clause principles.,"The first one uses SLIPPER to classify gaussian sluices, and the second one uses a memory-based machine learning procedure to classify bounded sluices from a training set.",The results show that the two algorithms perform well.
"We present a new metric for evaluating word similarity, called relative feature focus (RFF), which is a new criterion for evaluating word similarities.",This paper proposes a new metric for evaluating word similarity.,This paper proposes a new metric for evaluating word similarity measures.,The metric is based on the notion of entailment between two words.,The metric is based on the relative feature focus (RFF) and is independent of any particular vector similarity measure.,"The metric is based on the notion of entailment, which is a general notion of semantic similarity.",The metric measures the similarity between two words by comparing the features of two words.,The metric aims to improve the quality of feature vectors by a novel feature weighting function.,"The authors propose a new criterion for evaluating word similarities, based on the relative feature focus (RFF).","The RFF is a new measure for feature vector quality, which is independent of any particular vector similarity measure.",The metric improves the performance of word similarity measures by a significant margin.,The authors show that the metric is able to improve the performance of word similarity measures by a significant margin.,"To improve feature vector quality, a novel feature weighting function is introduced, called relative feature focal (RFF).",The metric also aims to improve word similarity by identifying the most important errors in the output of word similarity metrics.,The authors also show that the resulting metric can be used to improve the performance in a number of applications.,"To improve feature vectors, a novel weighting function is proposed, called relative feature attention (RFF).",We evaluate the quality of the RFF by comparing the similarity scores of two words in the same sentence.,We show that the quality of the metric is correlated with the quality of the feature vectors.,We present a new metric for evaluating the quality of word similarity measures.,This paper proposes a new metric for semantic-oriented NLP applications.,This paper proposes a new metric for evaluating word similarity measures.,The metric is based on the relative feature focus (RFF) and is independent of any particular vector similarity measure.,The metric is based on the relative feature focus (RFF) and is independent of any particular vector similarity measure.,"The metric is based on the notion of entailment, which is a general notion of semantic similarity.",The metric aims to improve the quality of feature vectors by a novel feature weighting function.,The metric aims to improve the quality of feature vectors by a novel feature weighting function.,"The authors propose a new criterion for evaluating word similarities, based on the relative feature focus (RFF).",The metric improves the performance of the metric by a significant margin.,The metric improves the performance of the metric by a significant margin.,The authors show that the metric is able to improve the performance of word similarity measures by a significant margin.,We propose a new criterion for evaluating the entailment relationship between words.,The metric also aims to improve performance of the resulting NLP applications.,The authors also show that the resulting metric can be used to improve the performance in a number of applications.
This paper presents a novel framework for machine transliteration.,This paper presents a novel framework for machine transliteration.,This paper proposes a new phoneme-based approach for machine transliteration.,The new framework carries out the transliteration by direct orthographical mapping (DOM) without any intermediate phonemic mapping.,The new framework carries out the transliteration by direct orthographical mapping (DOM) without any intermediate phonemic mapping.,The proposed method combines the phoneme-based method with a joint source-target phoneme-based model to model the source-target phonemic association.,"Under this framework, we further propose a joint source-channel transliteration mode (n-gram TM) as an alternative machine learning-based approach to model the source-target word orthographic association.","Under this framework, we further propose a joint source-channel transliteration mode (n-gram TM) as an alternative machine learning-based approach to model the source-target word orthographic association.",The proposed method is evaluated on English/Chinese and Japanese/English pairs.,"Without the loss of generality, we evaluate the performance of the proposed method for English/Chinese and English/Japanese pairs.","Without the loss of generality, we evaluate the performance of the proposed method for English/Chinese and English/Japanese pairs.",The results show that the proposed method outperforms other state-of-the-art methods.,An experiment that compares the proposed method with several state-of-the-art approaches is also also presented.,An experiment that compares the proposed method with several state-of-the-art approaches is also also presented.,The results show that our method outperforms other previous methods significantly.,The results show that our method outperforms other previous methods significantly.,We propose a method for transliteration that combines phoneme-based phoneme-based and phoneme-based approaches.,We propose a method for back-transliteration that combines phoneme-based phoneme-based and phoneme-based approaches.,This paper presents a novel framework for machine transliteration.,This paper presents a novel framework for machine transliteration.,This paper proposes a new phoneme-based approach for machine transliteration.,The new framework carries out the transliteration by direct orthographical mapping (DOM) without any intermediate phonemic mapping.,The new framework carries out the transliteration by direct orthographical mapping (DOM) without any intermediate phonemic mapping.,The proposed method combines the phoneme-based method with a joint source-target phoneme-based model to model the source-target phonemic association.,"Under this framework, we further propose a joint source-channel transliteration mode (n-gram TM) as an alternative machine learning-based approach to model the source-target word orthographic association.","Under this framework, we further propose a joint source-channel transliteration mode (n-gram TM) as an alternative machine learning-based approach to model the source-target word orthographic association.",The proposed method is evaluated on English/Chinese and Japanese/English pairs.,"Without the loss of generality, we evaluate the performance of the proposed method for English/Chinese and English/Japanese pairs.","Without the loss of generality, we evaluate the performance of the proposed method for English/Chinese and English/Japanese pairs.",The results show that the proposed method outperforms other state-of-the-art methods.,An experiment that compares the proposed method with several state-of-the-art approaches is also also presented.,An experiment that compares the proposed method with several state-of-the-art approaches is also also presented.,The results show that our method outperforms other previous methods significantly.,The results show that our method outperforms other previous methods significantly.,We propose a method for transliteration that combines phoneme-based phoneme-based and phoneme-based approaches.,We propose a method for transliteration that combines phoneme-based phoneme-based and phoneme-based approaches.
We present a model of interpretation of nominal compounds that accounts for structural rules of interpretation of nominal compound constituents.,We present a model of interpretation of nominal compounds that accounts for structural rules of interpretation of nominal compound constituents.,This paper proposes a new model of interpretation rules for nominal compounds.,"We describe a general model of interpretation that accounts for structural rule of interpretation of nominal constituents, i.e., domain-independent rnles that are deduced from the rnorpho-syntactic and semantic characteristics of the nominal constituents.",The model is based on general principles regarding the association between nouns and predicative information.,"The model is based on the notion of ""productivity"" and a ""semantic"" hierarchy, which is a set of semantic features that are adapted to the input of a neural network.",We then describe how this general model of interpretation can be used to enrich im formation retrieval in a system that is open-domaln.,"We then describe how the interpretation of nominal compounds is carried out when conlpoullds contain explicit predicative information, and then propose how this general model may be lied to the interpretation of compounds in texts.","The model is evaluated on a set of synthetic compounds, and a set of compound-specific datasets.","We describe es--> Phis project, which is supported by the CNET-INRIA project.",We describe a model of the interpretation of nominal compound compounds that accounts for the association between nounes and predicate information.,"The model is shown to be able to extract a large number of recursive nouns and compounds, and to be able for a variety of interpretation rules.",We describe linguistic rules for a compound that are a mixture of a recursive term and a recurrent term.,"We describe es--> Phis project, which is supported by the CNET-INRIA project.",We describe lexical features for a compound whose constituents are root nonainals.,We present a model of interpretation of nominal compounds that accounts for structural rules of interpretation of nominal compound constituents.,We present a model of interpretation of nominal compounds that accounts for structural rules of interpretation of nominal compound constituents.,This paper proposes a new model of interpretation rules for nominal compounds.,The model is based on general principles regarding the association between nouns and predicative information.,The model is based on general principles regarding the association between nouns and predicative information.,"The model is based on the notion of ""productivity"" and a ""semantic"" hierarchy, which is a set of semantic features that are adapted to the input of a neural network.","We then describe how the interpretation of nominal compounds is carried out when conlpoullds contain explicit predicative information, and then propose how this general model may be lied to the interpretation of compounds in texts.","We then describe how the interpretation of nominal compounds is carried out when conlpoullds contain explicit predicative information, and then propose how this general model may be lied to the interpretation of compounds in texts.","The model is evaluated on a set of synthetic compounds, and a set of compound-specific datasets.","We describe a model of the interpretation of nominal compound compounds that accounts for the structural rules of interpretation, and show how it can be used to enrich im formation retrieval in a system that is open-domaln.",We describe a model of the interpretation of nominal compound compounds that accounts for the structural rules of interpretation.,"The model is shown to be able to extract a large number of recursive nouns and compounds, and to be able for a variety of interpretation rules.","We describe es--> Phis project, a system that consists of a hierarchical hierarchy of nouns and constituents.","We describe es--> Phis project, which is supported by the CNET-INRIA project."
This paper presents results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.,This paper presents results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.,This paper studies the problem of identifying the speaker of a face-to-face meeting.,"The results are based on the Bayesian Network classifiers, which are able to identify the speaker and unaddressed participants.","The results are based on the Bayesian Network classifiers, which are based on a unified framework for identifying the participants in a face-to-few meeting.",The authors propose a Bayesian Network and Naive Bayes classifier for the task of addressee identification in four-participants face-to-few meetings.,The results are also used to compare the performance of the Bayesian and Naive Bayels classifiers for the task of addressee prediction over various feature sets.,The results are evaluated on four-partiance face-to-frame meetings with four participants.,The authors use a Bayes-based classifier to identify the speaker and unaddressed participants.,The results are obtained by comparing the performance of the two classifiers on four-partiance face-to- face meetings.,The results show that the Bayesian network and Naive Bayels classifiers perform better than the baselines.,The authors also use a Bayer-based classifier for the problem of addressee prediction.,"We use a Bayesian Network to identify the speaker, unaddress and ratified participants.",The results show that the Bayesian Network can identify the speaker and the unaddressing participants.,This paper presents results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.,This paper presents results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.,This paper studies the problem of identifying the speaker of a face-to-face meeting.,"The results are based on the Bayesian Network, a classifier for multi-modal information, and a classifier that combines information about meeting context with the performance of classifiers.","The results are based on the Bayesian Network, a classifier that is trained to predict the speaker's participation in the conversation.",The authors propose a Bayesian Network and Naive Bayes classifier for the task of addressee identification in four-participants face-to-few meetings.,"We use the Bayesian network to identify the speaker, addressee and unaddressed participants in a face-to-few meeting.",The results are also used to compare the performance of the Bayesian and Naive Bayels classifiers for the task of addressee prediction over various feature sets.,The authors use a Bayes-based classifier to identify the speaker and unaddressed participants.,We use the Naive Bayesian classifier to identify the speaker and unadjusted participants.,The results are shown to be useful for applications related to multi-party interaction.,The authors also use a Bayer-based classifier for the problem of addressee prediction.,The results show that the Bayesian Network can identify the speaker and the unaddressing participants.
We present a method for automatically identifying important sentences in a source text by determining their most likely argumentative role.,We present a method for automatically identifying important sentences in a source text by determining their most likely argumentative role.,This paper proposes a new method for automatically identifying important sentences in a text.,We use a corpus of scientific articles to generate a set of sentences that are characterized by their most likely argumentive role.,We use a corpus of scientific articles to generate a set of sentences that are characterized by their most likely argumentive role.,The method is based on the idea that rhetorical relations are global to the argumentation of the paper.,We use the corpus to generate a summary of scientific articles.,We use the corpus to generate a summary of scientific articles.,The authors propose to use a corpus of scientific articles to generate a corpus that is able to be used for summarization.,We use linguistic relations to determine the most likely argumentative roles in the paper.,We use linguistic relations to determine the most likely argumentative roles in the paper.,The corpus is then used to generate a summary of the paper.,We use lexical information to determine the most probable argumentative role in the paper.,We use lexical information to evaluate the annotation scheme's reliability.,We use supervised learning to evaluate the annotation schemes' reliability.,We present a method for automatically generating a summary of a scientific paper.,We present a method for automatically generating a summary of a scientific paper.,This paper proposes a new method for automatically identifying important sentences in a text.,"The method is based on a corpus of scientific articles, which are a collection of texts that are annotated with information about the argumentative role that sentence plays in the paper.","The method is based on a corpus of scientific articles, which are a collection of texts that are annotated with information about the argumentative role that sentence plays in the paper.",The method is based on the idea that rhetorical relations are global to the argumentation of the paper.,The annotation scheme is based on the notion of rhetorical relations as defined by Rhetorical Structure Theory (RST).,The annotation scheme is based on the notion of rhetorical relations as defined by Rhetorical Structure Theory (RST).,The authors propose to use a corpus of scientific articles to generate a corpus that is able to be used for summarization.,"The annotation scheme is tested on a range of text types, including research articles, scientific articles, and scientific articles.","The annotation scheme is tested on a range of text types, including research articles, scientific articles, and scientific articles.",The corpus is then used to generate a summary of the paper.,"We use a corpus to generate a summary of the paper, and use it to generate a summary of the paper.",We show that the annotation scheme can be used for text summarization in a range of fields.,We use linguistic techniques to evaluate the annotation scheme's reliability.,"We use a corpus to generate a summary of scientific articles, and use it to generate a summary of scientific articles.",We use lexical information to evaluate the annotation of a sentence.
We present a new corpus of bilingual text pairs aligned at various levels of granularity.,This paper presents a new corpus of bilingual text pairs aligned at various levels of granularity.,This paper presents a new resource for mining bilingual text from the Web.,We use the bilingual laws information system (BLIS) to extract the bilingual text pairs from the Web and extract the content texts from the Web pages.,"The bilingual text pairs are extracted from the Web, and the alignment results are based on the HK-SAR.",The resource is based on the bilingual laws information system (BLIS) and is based on a bilingual corpus of HK laws.,We use the BLIS to extract the text structures from the Web pages and align them using consistent intrinsic features in the Web pages and content texts.,The bilingual text pair is extracted from the Web and aligned with the text structure.,The resource is extracted from the BLIS and a sub-paragraph alignment is performed.,We use the XML schema to encoding the alignment results and illustrate the display mode for browsing the aligned bilingual corpus.,"The alignment results are compared to the HK-sAR, and the HK-BLIS, which is a bilingual information system.",The results are compared to the BLIS and the HK-SAR.,We use the Bilingual Law Information System (BLIS) for extracting the bilingual text pairs.,The HK-SARS is a bilingual text pair that is extracted from the web.,The results show that the resource is useful for a variety of applications.,The HK HK-Sar is a bilingual lexicon of the laws of Hong Kong.,The HK is a bilingual corpus of the laws of the laws of HK.,The HK's laws are a sub-paragraph-aligned bilingual corpus of about 20 million words.,The HK laws are a bilingual text corpus of the law of Hong Kong.,"Bitexts, also referred to as parallel texts or bilingual corpora, are a valuable resource for empirical machine translation research.","We present a new bilingual corpus of parallel texts of the laws of Hong Kong, which is a subparagraph-aligned bilingual corpus of about 20 million words.",This paper presents a new resource for mining bilingual text from the Web.,"We present a new bilingual text alignment system, which is based on the bilingual laws information system (BLIS), which is based upon the bilingual laws information systems (BLIS).","The bilingual texts of the laws are introduced in Section 2, with an emphasis on HK's legislation text hierarchy and its numbering system that can be utilized for text alignment to subparagraph level.",The resource is based on the bilingual laws information system (BLIS) and is based on a bilingual corpus of HK laws.,We use the BLIS to extract the bilingual text from the Web and extract the content texts from the Web.,The bilingual texts are also characterized by consistent intrinsic features in the Web pages and content texts.,The resource is extracted from the BLIS and a sub-paragraph alignment is performed.,We use the BLI to extract the content texts and align the text structures in terms of the text hierarchy.,We use the bilingual laws information system to extract the bilingual text from the Web and extract the content texts from the Web pages.,The results are compared to the BLIS and the HK-SAR.,We use the XML schema to encoding the alignment results and illustrate the display mode for browsing the aligned bilingual corpus.,We use the XML schema for encoding the alignment results and illustrate the display mode for browsing the aligned bilingual corpus.,The results show that the resource is useful for a variety of applications.,We use the bilingual laws information corpus to extract the bilingual texts from the Web and align the text structure in terms of the numbering system.
We present a method for identifying parallel sentences in comparable corpora.,We present a method for identifying parallel sentences in comparable corpora.,This paper proposes a method for identifying parallel sentences in comparable corpora.,We use a maximum entropy classifier to determine whether two sentences are translations of each other.,We use a maximum entropy classifier to determine whether two sentences are translations of each other.,The method is based on the idea of entropy-based classifiers that can reliably identify good translations of parallel texts.,We use reorderings and deletions to determine whether two sentence pairs are translations of one another.,We use reorderings and deletions to determine whether two sentence pairs are translations of one another.,"The method is evaluated on a large dataset of newspaper articles, and shows that the method can improve the performance of a baseline MT system trained only on existing parallel data.",We use entropy-based classifiers to determine whether sentences are translations.,We show that adding these sentences to the training data improves the system's performance.,We use lexical information retrieval to determine whether two pairs are translations.,We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method.,We use entropy-based classifiers to determine whether sentences are translations.,We present a method for identifying parallel sentences in comparable corpora.,This paper describes a method for identifying parallel sentences in comparable corpora.,This paper proposes a method for identifying parallel sentences in comparable corpora.,We use a maximum entropy-based classifier to determine whether two sentences are translations of each other.,We use a maximum entropy-based classifier to determine whether two sentences are translations of each other.,The method is based on the idea of entropy-based classifiers that can reliably identify good translations of parallel texts.,We use this classifier to extract parallel sentences from very large comparable corpora of newspaper articles.,We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method.,"The method is evaluated on a large dataset of newspaper articles, and shows that the method can improve the performance of a baseline MT system trained only on existing parallel data.",We show that adding these sentences to the training data of an SMT system improves the performance of the system.,We use entropy based classifiers to determine whether two sentence pairs are translations of one another.,We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method.,We use reorderings and deletions to determine whether the sentences are translations.,We use entropy based classifiers to determine whether two sentence pairs are translations of one another.
This paper presents a tree kernel based approach for relation extraction.,This paper presents a tree kernel based approach for relation extraction.,This paper proposes a tree kernel based approach for relation extraction.,The tree kernel is a combination of a dependency tree kernel and a dependency tree.,"The method uses a convolution tree kernel to capture syntactic, syntactic and semantic features.","The proposed method is based on the idea of combining the syntactic, syntactic and semantic features.",The dependency tree kernel is based on the tree-level information of the parse tree.,The experiments show that the convolution tree kernel plus entity features achieves slightly better performance than the best-reported feature-based methods.,The authors show that the convolution tree kernel and entity features achieves slightly better performance than the two dependency tree kernels.,The dependency trees are a combination of the tree-level and dependency tree features.,"It also shows that our method significantly outperforms the two dependency tree kernels (Bunescu and Mooney, 2005) on the 5 ACE relation types.",The experiments show that the conv tree kernel and entity feature achieves slightly better results than the two dependency trees.,The dependency tree features are a combination between the tree-level features and the dependency tree features.,We use a dependency tree kernel to capture the syntactic tree features and other various features that have been proven useful in feature-based methods for relation extraction.,The experiments also show that the conc tree kernel and entity tree kernel achieves slightly better performances than the two dependency Tree kernels.,We show that the dependency tree kernel and the dependency tree kernel achieve slightly better performance than the best-reported feature-based methods.,We also show that the convolution tree kernel plus entity features achieves slightly better performance than previous best-reported features.,We use a dependency tree tree kernel to extract the syntactic tree features.
"This paper presents an empirical study of the role of nonverbal information in grounding, and presents an implementation of that model into an embodied conversational agent.","This paper presents an empirical study of the role of nonverbal information in grounding, and proposes a model of grounding using both verbal and nonverbal information.",This paper proposes a model of grounding that uses both verbal and nonverbal information to ground the grounding process.,"The model is based on the observation that the listener is actively observing the listener's behavior, and that the listener' s attention is primarily on the task of grounding.","The model is based on the observation that the listener is actively listening to the signal of grounding, and that the listener's attention is conditioned on the task.","The model is based on the observation that the listener is actively listening to the grounding signal, and that the listener's attention is a signal of understanding of the grounding process, and that the grounding process is a collaborative process.","The model is evaluated on a set of human face-to-face datasets, and compared to a baseline model of grounding using both verbal and nonverbal information.","The model is evaluated on a human face-to-face dataset, and compared with a embodied conversational agent.",The authors show that the model can be applied to embodied humanoids that can engage in human conversational protocols.,We show that the model of grounding is able to establish common grounding using both visual and verbal information.,"The model is tested on a user interacting with an embodied conversation agent, and compared against a user observing with an embodieded conversational agent, and a embodieded humanoid.","This paper presents an empirical study of the role of nonverbal information in grounding, and presents an implementation of that model into an embodied conversational agent.","This paper presents an empirical study of the role of nonverbal information in grounding, and proposes a model of grounding using both verbal and nonverbal information.",This paper proposes a model of grounding that uses both verbal and nonverbal information to ground the grounding process.,"The model is based on the observation that the listener is actively observing the listener's behavior, and that the listener' s attention is primarily on the task of grounding.","The model is based on the observation that the listener is actively listening to the signal of grounding, and that the listener's attention is conditioned on the task.","The model is based on the observation that the listener is actively listening to the grounding signal, and that the listener's attention is a signal of understanding of the grounding process, and that the grounding process is a collaborative process.","The model is evaluated on a variety of datasets, including a human-machine interface, a human-dog interface, and a simulated human-dog interface.","The model is evaluated on a human face-to-face dataset, and compared with a embodied conversational agent.",The authors show that the model can be applied to embodied humanoids that can engage in human conversational protocols.,We show that the model can be used to adapt dialogue management to face-to-face conversation with an embodied humanoid.,"The model is tested on a user interacting with an embodied conversation agent, and compared against a user observing with an embodieded conversational agent, and a embodieded humanoid."
We present a method for automatically acquiring topic signatures for WSD.,This paper presents a method for automatically acquiring topic signatures for WSD.,This paper proposes a method for automatically acquiring topic signatures for WSD.,"We use Chinese-English and English-Chinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora.","The method builds topic signatures by using Chinese-English and English-Chinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora.","The method is based on a cross-lingual paradigm, where the goal is to generate a set of topic signatures that are similar to the word in the other language.","Since topic signatures are potentially good training data for WSD algorithms, we set up a task to disambiguate 6 words using a WSD algorithm similar to Schütze's 1998 context-group discrimination.","The method is based on a cross-lingual paradigm, which is a cross-linguistic paradigm.","The method is evaluated on a set of WSD tasks, and the results show that the proposed method is effective in achieving the goal.",The results show that our topic signatures are useful for WSD.,The method is evaluated on a task to disambiguate 6 words using a WSD algorithm similar to Schütze's context-group discrimination.,The results show that our topic signatures are useful for WSD.,We use a cross-language model to generate topic signatures.,We present a method for automatically acquiring topic signatures for WSD.,This paper presents a method for automatically acquiring topic signatures for WSD.,This paper proposes a method for automatically acquiring topic signatures for WSD.,"We use Chinese-English and English-Chinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora.","The method builds topic signatures by using Chinese-English and English-Chinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora.","The method is based on a cross-lingual paradigm, where the goal is to generate a set of topic signatures that are similar to the word in the other language.","Since topic signatures are potentially good training data for WSD algorithms, we set up a task to disambiguate 6 words using a WSD algorithm similar to Schütze's 1998 context-group discrimination.","Since topic signatures are potentially good training data for WSD algorithms, we set up a task to disambiguate 6 words using a WSD algorithm similar to Schütze's 1998 context-group discrimination.","The method is evaluated on a set of WSD tasks, and the results show that the proposed method is effective in achieving the goal.",The results show that our topic signatures are useful for WSD.,The results show that our topic signatures are useful for WSD.,The paper describes the acquisition process of automatically acquiring topic Signatures.,We use a cross-lingual paradigm to automatically acquire topic signatures.,We use Chinese monolingual text to generate topic signatures.,"We use a cross-lingual paradigm to automatically generate topic signatures, which is a cross-linguistic paradigm.",We use Chinese bilingual text to generate topic Signatures.
"We present a new unlexicalized parser for German, which achieves the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech tags from the input text.","We present a new unlexicalized parser for German, which achieves the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech tags from the input text.",This paper presents a new unlexicalized parsing model for German.,"We also use treebank transformations ( Johnson, 1998; Klein and Manning, 2003) instead of a grammar induced directly from a treebank.",We also use treebank transformations instead of a grammar induced directly from a treebank.,"The model is based on a treebank transformation, which is a grammatical function of the underlying word order.","We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present.","We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present.",The authors show that the resulting parser is able to achieve state-of-the-art results in German.,"We use three different approaches to improve the performance of unlexicalized German parsing, allowing us to compare the relative performance of each.","We use three different approaches to improve the performance of unlexicalized German parsing, allowing us to compare the relative performance of each.",The authors also provide a comparison with other unlexicalized models.,We use a treebank transformation to extract a grammatical function from the grammatical functions of the two NPs above.,We use a treebank transformation to extract a grammatical function from the grammatical functions of the two NPs above.,The authors also show that the derived parser is more accurate than the derived parses.,We present a parser for German that achieves the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech tags from the input text.,We present a parser for German that achieves the highest performance known to us by making use of smoothing and a highly-tuned suffix analyzer for guessing part-of-speech tags from the input text.,This paper presents a new unlexicalized parsing model for German.,We also use treebank transformations instead of a grammar induced directly from a treebank.,We also use treebank transformations instead of a grammar induced directly from a treebank.,"The model is based on a treebank transformation, which is a grammatical function of the underlying word order.","We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present.","We show that suffix analysis is not helpful on the treebank grammar, but it does increase performance if used in combination with the treebank transformations we present.",The authors show that the resulting parser is able to achieve state-of-the-art results in German.,"We use three different approaches to improve the performance of unlexicalized parsing, allowing us to compare the relative performance of each.","We use three different approaches to improve the performance of unlexicalized parsing, allowing us to compare the relative performance of each.",The authors also provide a comparison with other unlexicalized models.,"We use a treebank transformation to extract a grammatical function from the input text, and use it to extract a dependency function from the input.",We use a treebank transformation to extract a grammatical function from the grammatical functions of the two NPs above.,The authors also show that the derived parser is more accurate than the derived parses.
"In this paper, we investigate unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.","In this paper, we investigate unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.",This paper investigates unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.,"We find that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","We find that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","The authors show that unsupervised methods can achieve accuracies comparable to those achieved by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","In both domains, we find that unsupervised approaches can attain accuracy with 400 unlabelled examples comparable to the ones attained by a supervised method on 50 labeleted examples, and that Semi-supervised methods can be good use of small quantities of labeled examples.","In both domains, we find that unsupervised approaches can attain accuracy with 400 unlabelled examples comparable to the ones attained by a supervised method on 50 labeleted examples, and that Semi-supervised methods can be good use of small quantities of labeled examples.",The authors also show that semi-supervised approaches can make good use for small amounts of label data.,"We propose a novel unsupervised method for field segmentation, which is based on a hidden Markov model.","We propose a novel unsupervised method for field segmentation, which is based on a hidden Markov model.","In this paper, we investigate unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.",This paper investigates unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.,This paper investigates unsupervised learning of field segmentation models in two domains: bibliographic citations and classified advertisements for apartment rentals.,"We find that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those achieved by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","In both domains, we find that unsupervised methods can attain accuracies comparable to those achieved by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","The authors show that unsupervised methods can achieve accuracies comparable to those achieved by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.","In both domains, we find that unsupervised approaches can attain accuracy with 400 unlabelled examples comparable to the ones attained by supervised methods.","In both domain, we find that small amounts of prior knowledge can be used to greatly improve the learned model.",The authors also show that semi-supervised approaches can make good use for small amounts of label data.,"In both domain, we find that small amounts of prior knowledge can be used to greatly improve the learned model.","In both domain settings, we find that unseen fields present a salient form of linguistic structure, and that unsupervised methods are able to attain accuraces with 400 unlabeled examples comparable to those achieved using supervised methods on 50% labeled examples.",In both domain we find that unconstrained induction of HMMs using the EM algorithm fails to detect useful field structure in either domain.,"We propose a novel unsupervised learning method for field segmentation, which is based on a supervised learning approach.","However, we demonstrate that small amounts of knowledge can be used for greatly improving the learned model.",We use a supervised HMM to learn a supervised model for a document that is regarded as a sequence of relevant fields.
"This paper presents a new model for sentence boundary detection, based on a maximum entropy (Maxent) posterior classification method.",This paper presents a new model for sentence boundary detection.,This paper proposes a novel approach to segmentation of sentences.,The model is based on generative modeling of the hidden variables of the HMM system.,The model is based on a maximum entropy (Maxent) posterior classification method.,The proposed approach is based on a maximum entropy (Maxent) posterior classification method.,"The model is trained using a decision tree classifier, and the model is trained using the supervised learning of the transition probabilities.",The model is trained using a generative model that is based on the N-gram language model.,The authors propose to use a generative model to model the underlying structure of the sentence boundary.,"The model is evaluated on the task of sentence boundary detection, and the results show that the proposed model outperforms the baselines.","The model is evaluated on a set of tasks, including sentence boundary detection, tagging, and summarization.",The authors show that the proposed approach can achieve better performance than the baselines.,We propose a new model that uses a maximum entangled generative model to model the hidden variables of a sentence boundary.,The model is shown to outperform the baselines.,The authors also show that the proposed method can achieve better performance compared to the baselines.,This paper presents a novel approach to sentence boundary detection.,This paper presents a novel approach to sentence boundary detection.,This paper proposes a novel approach to segmentation of sentences.,The approach is based on a maximum entropy (Maxent) posterior classification method.,The approach is based on a maximum entropy (Maxent) posterior classification method.,The proposed approach is based on a maximum entropy (Maxent) posterior classification method.,"The approach is evaluated on a set of datasets, including a tagging task, a speech prosody task, and a tying task.","The approach is evaluated on a set of datasets, and the results show that the proposed approach outperforms the baselines.",The authors propose to use a generative model to model the underlying structure of the sentence boundary.,The approach is tested on a set containing a large number of words and a large number tagged sentences.,"The approach is also evaluated on a number of datasets, including a digit-based dataset, a digit based dataset, and a digit graph dataset.",The authors show that the proposed approach can achieve better performance than the baselines.,The approach is shown to outperform the baselines.,The authors also show that the proposed method can achieve better performance compared to the baselines.,We present a method for detecting the boundary position of a sentence in a sentence.
This paper presents a new self-supervised method for machine translation.,This paper presents a new self-supervised method for machine translation.,"This paper proposes a new model for machine translation, which is a combination of bidirectional and auto-regressive transformers.",BART is a sequence-to-sequence model that is applicable to a wide range of end tasks.,"The method is based on a bidirectional encoder, which is trained to predict the next sentence.",The model is trained using a bidirectional encoder and a bidirectional decoder.,"BART uses a standard Tranformer-based neural machine translation architecture which combines bidirectional encoder, GPT, and many other recent pretraining schemes.","The encoder is trained to predict a mask token, and the decoder is trained to generate the next sentence.",The model is then trained using a sequence-to-sequence model.,The BART model is a combination of bidirectional encoder and auto-regressive transformers.,"This method is evaluated on a range of tasks, including abstractive dialogue, question answering, and summarization.","The model is evaluated on GLUE, SQuAD, and SQuAD.",The BART encoder is a bidirectional encoder that is trained to predict the next sentence.,"The method is evaluated on three tasks: abstractive dialogue, Question Answering, and summarizing.","The BART outputs are a sequence of arbitrary transformations of the original text, including shuffling the order of the original sentences and using a novel in-filling scheme.",The results show that the proposed method is competitive with other self-supervised methods.,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.","We propose a new self supervised method for machine Translation, which pre-trains a BART encoder with a sequence- to sequence model that is trained to reconstruct the original text.","This paper presents BART, a new method for pre-training a sequence-to-sequence model that is applicable to a wide range of end tasks.","This paper presents BART, a new method for pre-training a sequence-to-sequence model that is applicable to a wide range of end tasks.","This paper proposes a new model for machine translation, which is a combination of bidirectional and auto-regressive transformers.",BART is a sequence-based auto-regressive model that is applicable for a very wide range of end-tasks.,BART is a sequence-based auto-regressive model that is applicable for a very wide range of end-tasks.,The model is trained using a bidirectional encoder and a bidirectional decoder.,"Pretraining has two stages: text is corrupted with an arbitrary noising function, and a sequence- to-sequencing model is learned to reconstruct the original text.","BART uses a standard Tranformer-based neural machine translation architecture which despite its simplicity, can be seen as generalizing BERT.",The model is then trained using a sequence-to-sequence model.,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.,"The key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length.","The model is evaluated on GLUE, SQuAD, and SQuAD.","It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.",This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.,BART also opens up new ways of thinking about fine tuning.,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.,We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.",We use a bidirectional encoder to encode the next sentence and use a mask token to replace the spans of the text.,BART also opens up new ways of thinking about fine tuning.,"BART is also a new scheme for thinking about fine tuning, and achieves state-of-art results on abstractive dialogue, Question answering, and summarisation tasks.",BART achieves state-Of-the-art
"We present XLM-R, a transformer-based multilingual masked language model pre-trained on text in 100 languages, which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering.","This paper presents a comprehensive analysis of the trade-off between high-resource and low-resource languages, and the impact of language sampling and vocabulary size.",This paper presents a comprehensive analysis of the tradeoff between high-resource and low-resource languages and the impact of language sampling and vocabulary size.,"We first show that the trade-off between high-resource and low-resource languages is a tradeoff between high- resource and low-reresource languages, and show that it can be alleviated by simply increasing model capacity.","The experiments expose a trade-off between higher-resource and lower-resource languages and the impact of word sampling and vocabulary size, and show that it can be alleviated by simply increasing model capacity.","The authors show that the tradeoff between higher-resource and high-resource languages is a curse of multilinguality, and propose a new model XLM-RoBERTa, which is a transformer-based multilingual masked language model.","We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-RR achieves performance competitive with state-of-art monolingual models, including RoBERTa.","We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-R outperforms mBERTa on cross-lingual classification by up to 23 % accuracy on low-resource language.",The authors also show that the proposed model can be used to improve the performance of the existing state-of-the-art multilingual models.,We show that the tradeoff between high resource and low-Resource languages is bounded by increasing model capacity.,"We also evaluate the monolingual fine tuning of XLM-r on the GLEU and XNLi benchmarks, where we obtain results competitive with state-of-the-art monolingual models.",The authors also propose a new benchmark for cross-lingual NLP and cross-lingual Question Answering.,"We present XLM-R, a transformer-based multilingual masked language model pre-trained on text in 100 languages, which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering.","This paper presents a comprehensive analysis of the trade-off between high-resource and low-resource languages, and the impact of language sampling and vocabulary size.",This paper presents a comprehensive analysis of the tradeoff between high-resource and low-resource languages and the impact of language sampling and vocabulary size.,"We first show that the trade-off between high-resource and low-resource languages is a tradeoff between high- resource and low-reresource languages, and show that it can be alleviated by simply increasing model capacity.","The experiments expose a trade-off between higher-resource and lower-resource languages and the impact of word sampling and vocabulary size, and show that it can be alleviated by simply increasing model capacity.","The authors show that the tradeoff between higher-resource and high-resource languages is a curse of multilinguality, and propose a new model XLM-RoBERTa, which is a transformer-based multilingual masked language model.","We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-RR achieves performance competitive with state-of-art monolingual models, including RoBERTa.","We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-R outperforms mBERTa on cross-lingual classification by up to 23 % accuracy on low-resource language.",The authors also show that the proposed model can be used to improve the performance of the existing state-of-the-art multilingual models.,We show that the tradeoff between high resource and low-Resource languages is bounded by increasing model capacity.,"We also evaluate the monolingual fine tuning of XLM-r on the GLEU and XNLi benchmarks, where we obtain results competitive with state-of-the-art monolingual models.",The authors also propose a new benchmark for cross-lingual NLP and cross-lingual Question Answering.
This paper presents a library of pre-trained NLP models for natural language processing.,This paper presents a library of pre-trained NLP models for natural language processing.,This paper introduces a new library called 'Target' for storing and storing pre-trained models.,The library supports the distribution and usage of a wide variety of pre-tuned models in a centralized model hub.,The library supports the distribution and usage of a wide variety of pretrained models in a centralized model hub.,"The library is built on top of the tensor2tensor library, which is a repository of popular models for NLP tasks.",This library supports the distribution of pre-tailed models in a central model hub.,"This library supports the distribution of a wide-variety of pretrained models, including models from Marian NMT and Google's BERT.","The library is a repository for NLP tasks, and is a repository that supports the distribution and usage of pre-trained model models.",This repository supports the distribution and use of a wide range of pre-tried models in a large-variety of pre-teaching models in a comprehensive and centralized model library.,"The library supports easy download, caching, and fine-tuning of the models as well as seamlessly transition to production.","The library contains a variety of popular models, including BERT, Open-NMT, Texar, Megatron-LM, and Marian NMT.",The library supports both research and deployment of a wide-variety of pretrained models in a single package.,The library is released under the Apache 2.0 license and is available on GitHub.,The library is open-sourced and is open-sourced for NLP tasks and NLP tasks.,"This library supports both research, analysis, and deployment of arbitrary pre-trained models in centralized model hubs.",The library is a repository of pre-tuned NLP models.,"This library supports easy download, caching, and fine-tuning of the models as well as seamlessly transition to production.",The library is designed to support the distribution and usage a wide variety  of pretrained models.,The library is released under the Apache 2.0 license and is available on GitHub.,The library supports both research and production.,The library is designed to support the distribution and usage a wide-variance of pre-tened models in a distributed model hub.,"The library supports industrial-strength implementations of popular model variants that are easy to read, extend, and compress.",This hub supports users to compare different models with the same minimal API and to experiment with shared models on a variety of different tasks.,The library supports users to compare different models with the same minimal API and to experiment with shared models on a variety of different tasks.,"The library is built on top of the tensor2tensor library, which is also closely related to neural translation and language modeling systems.",The library is built on top of the tensor2tensor library.,The library supports a wide variety a wide variety (and often large,The library is open source and easy to use.,This paper presents a library of pre-trained Transformer models for both natural language understanding and natural language generation.,This paper presents a library of pre-trained NLP models for both natural language understanding and natural language generation.,This paper introduces a new library called 'Target' for storing and storing pre-trained models.,The library supports the distribution and usage of a wide variety of pre-tuned models in a centralized model hub.,The library supports the distribution and usage of a wide variety of pretrained models in a centralized model hub.,"The library is built on top of the tensor2tensor library, which is a repository of popular models for NLP tasks.",This library supports the distribution of pre-tailed models in a central model hub.,"This library supports the distribution of a wide-variety of pretrained models, including models from Marian NMT and Google's BERT.","The library is a repository for NLP tasks, and is a repository that supports the distribution and usage of pre-trained model models.",This repository supports the distribution and use of a wide range of pre-tried models in a large-variety of pre-teaching models in a comprehensive and centralized model library.,"The library supports easy download, caching, and fine-tuning of the models as well as seamlessly transition to production.","The library contains a variety of popular models, including BERT, Open-NMT, Texar, Megatron-LM, and Marian NMT.",The library supports both research and deployment of a wide-variety of pretrained models in a single package.,The library is released under the Apache 2.0 license and is available on GitHub.,The library is open-sourced and is open-sourced for NLP tasks and NLP tasks.,"This library supports both research, analysis, scale, and augment the model on a variety of platforms.",The library is a repository of pre-tuned NLP models.,"The library supports industrial-strength implementations of popular model variants that are easy to read, extend, and deploy.",The library is designed to support the distribution and usage a wide variety  of pretrained models.,"On this foundation, the library supports the distribution, and usage of arbitrary pre-trained models in centralized model hub, which supports users to compare different models with the same minimal API and to experiment with shared models on a variety different tasks.",The library supports both research and production.,Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors.,"The library supports industrial-strength implementations of popular model variants that are easy to read, extend, and compress.",The library is released under the Apache 2.0 license and is available on GitHub.,The library supports users to compare different models with the same minimal API and to experiment with shared models on a variety of different tasks.,The Transformers library is designed to support Transformer-based architectures and facilitating the distribution,The library is built on top of the tensor2tensor library.,The library is open source and easy to use.
"This paper introduces a new dataset, MultiGenre NLI, which is a new benchmark for natural language inference.","This paper introduces a new dataset, MultiGenre NLI, which is a new benchmark for natural language inference.","This paper introduces a new dataset, MultiNLI, for NLI evaluations.","The dataset is based on the Stanford NLI corpus, which is the only large human-annotated corpus for NLI.","The dataset is based on the Stanford NLI corpus, which is based on a large-scale NLI evaluation.","The dataset is based on the Stanford NLI corpus, which is a large human-annotated corpus.",The dataset is designed to be able to run large-scale NLI evaluations that capture more of the complexity of modern English.,The dataset is designed to be able to run large-scale N LI evaluations that capture more of the natural language understanding (NLU) process.,"The dataset is designed to be able to run large-scale NLI evaluations, which is based on a large-scale NSL evaluation task.","The dataset is built on top of the Stanford NLI Corpus, which is based on a large-scale NLP task.",The dataset is also used to evaluate the performance of a number of NLI models.,"The dataset is built on top of the Stanford NLI dataset, which is modeled closely on the Stanford NSL corpus.",The dataset is used to evaluate the performance of a number of NLI models on a variety of NLI tasks.,The dataset is used to evaluate the best performance of a few NLI models.,"The paper also introduces a dataset called MultiNLI, which is adapted from the Stanford NLI data set to provide a large-sized dataset for NLI evaluation.",The dataset is also used to evaluate the quality of the representations learned by the models.,The dataset is used to evaluate NLI evaluations on NLI tasks.,"We present a new dataset called MultiGenre N LI, which is aimed at evaluating the performance of ML models on NLI tasks.","This paper introduces a new dataset, MultiGenre NLI Corpus (MultiNLI), which is a large-scale NLI evaluation dataset for NLI.","This paper introduces a new dataset, MultiGenre NLI, to evaluate the performance of NLI models.","This paper introduces a new dataset, MultiNLI, for NLI evaluations.","The dataset is based on the Stanford NLI Corpus, which is a very large human-annotated corpus for NLI.","The dataset is based on the Stanford NLI Corpus, which is a large human-annotated corpus for NLI.","The dataset is based on the Stanford NLI corpus, which is a large human-annotated corpus.","This dataset is a good source of information for NLI research, and is a good starting point for future research on NLI.",The dataset is designed to be able to run large-scale NLI evaluations that capture more of the complexity of modern English.,"The dataset is designed to be able to run large-scale NLI evaluations, which is based on a large-scale NSL evaluation task.","We present a new dataset called MultiGenre N LI Corpus (MultiNLI), which is based on a large-sized dataset of NLI evaluations.","The dataset is used to evaluate the performance on NLI tasks, and the authors present a new benchmark dataset, MultiNLI, which is based on a large-scale N LI evaluation.","The dataset is built on top of the Stanford NLI dataset, which is modeled closely on the Stanford NSL corpus.","We present supervised learning methods for NLI evaluations, and show that the multiGenre NLI corpus can be used to evaluate NLI models on a large scale NLI evaluation dataset.",The dataset is also used to evaluate the model performance on NLI benchmarks.,"The paper also introduces a dataset called MultiNLI, which is adapted from the Stanford NLI data set to provide a large-sized dataset for NLI evaluation.",The dataset is used to evaluate NLI evaluations on NLI tasks.
"We present a new type of contextualized word representation that directly addresses both challenges, can be easily integrated into existing NLP models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.","This paper introduces a new type of contextualized word representation that directly addresses both challenges, can be easily integrated into existing NLP models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.","This paper introduces ELMo, a new method for learning contextualized word representations.",Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.,"The method is based on a bidirectional LSTM model, which is trained with a LM objective.",We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text corpus.,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text corpus.,"The authors show that the proposed method can be applied to existing models for textual entailment, question answering and sentiment analysis.","For this reason, we call them ELMo (Embeddings from Language Models) representations.","For this reason, ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.","Experiments on textual entitlement, question answering, and sentiment analysis show that the proposed approach outperforms the baselines.","Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning, and lower-level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).",Combining the internal states in this manner allows for very rich word representations.,"Simultaneously exposing all these signals is highly beneficial, allowing the learned models to select the types of semi-supervision that are most useful for each end task.","Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning, and lower-level states model aspects of syntax.",Experiments demonstrate that ELMo representations outperform those derived from just the top layer of an LSTM.,"Simultaneously exposing all these signals is highly beneficial, allowing the learned models to select the types of semi-supervision that are most useful for each end task.","We use a bidirectional language model to learn word vectors, and use a bi-directional LSTM to learn word vector representations.","Experiments demonstrate that ELMo representation improves the state-of-the-art in all cases, including up to 20 % relative error reductions.",We use a bidirectional language model to learn word vectors.,"We present a new type of contextualized word representation that directly addresses both challenges, can be easily integrated into existing NLP models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.","This paper introduces a new type of contextualized word representation that directly addresses both challenges, can be easily integrated into existing NLP models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.","This paper introduces ELMo, a new method for learning contextualized word representations.",Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.,"The method is based on a bidirectional LSTM model, which is trained with a LM objective.",We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text corpus.,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text corpus.,"The authors show that the proposed method can be applied to existing models for textual entailment, question answering and sentiment analysis.","For this reason, we call them ELMo (Embeddings from Language Models) representations.","For this reason, ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.","Experiments on textual entitlement, question answering, and sentiment analysis show that the proposed approach outperforms the baselines.","Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning, and lower-level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).",Combining the internal states in this manner allows for very rich word representations.,"Simultaneously exposing all these signals is highly beneficial, allowing the learned models to select the types of semi-supervision that are most useful for each end task.","Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning, and lower-level states model aspects of syntax.",Experiments demonstrate that ELMo representations outperform those derived from just the top layer of an LSTM.,"Simultaneously exposing all these signals is highly beneficial, allowing the learned models to select the types of semi-supervision that are most useful for each end task.","We use a bidirectional language model to learn word vectors, and use a bi-directional LSTM to learn word vector representations.","Experiments demonstrate that ELMo representation improves the state-of-the-art in all cases, including up to 20 % relative error reductions.",We use a bidirectional language model to learn word vectors.
We present a new pre-training method for language representations.,This paper presents a new pre-training method for language representations.,This paper proposes a new method for pre-training language representations.,"The method is inspired by the Cloze task, which is a task-specific task that requires pre-trained language models to perform well.","The method is inspired by the Cloze task, which is a task-specific task.",The proposed method is based on a masked language model (MLM) pre-training objective.,"The method is based on a masked language model, which randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.","The method is based on a masked language model, which is masked with a mused context mask.",The masked language models are used to predict the original vocabulary id of the masked word.,"The method is evaluated on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.","The method is evaluated on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",The authors show that the proposed method can achieve state-of-the-art performance on a range of tasks.,We show that the pre-trained representations reduce the need for many heavily-engineered task-specific architecture.,The method achieves state-of-the-art performance on a large set of NLP tasks.,"We propose a bidirectional encoder representation model that is able to achieve state-of-the-art performance on a large set of sentence-level tasks, and outperforms many task-specific baselines.",The method is also evaluated on a few NLP tasks.,We present a new fine-tuning based approach for language representations.,This paper presents a new fine-tuning based approach for language representations.,This paper proposes a new method for pre-training language representations.,"The approach is based on a masked language model (MLM) pre-training objective, inspired by the Cloze task.","The approach is based on a masked language model (MLM) pre-training objective, inspired by the Cloze task.",The proposed method is based on a masked language model (MLM) pre-training objective.,"The MLM objective is a combination of the left and right contexts, which allows the model to fuse the left and right contextual contexts.","The MLM objective is a combination of the left and right contexts, which allows the model to fuse the left and right contextual contexts.",The masked language models are used to predict the original vocabulary id of the masked word.,The MLM pre-training objective is a mailed language model pre-training objective.,"The approach achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.",The authors show that the proposed method can achieve state-of-the-art performance on a range of tasks.,"In addition to the masked language modeling objective, we also use a next sentence prediction task that jointly pretrains text-pair representations.",The paper also presents a next sentence prediction task that jointly pretrains text-pair representations.,We show that the pre-trained representations reduce the need for many heavily-engineered task-specific architectures.,The results show that the proposed approach achieves state of the art performance on a wide suite of sentence-based and token-level NLP tasks.,We use a bidirectional language model to pretrain a bidirectional encoder representation.
"Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling.","Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling.",This paper presents a new toolkit for a sequence modeling toolkit.,"In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production.","In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production.",The key idea is to extend the baseFairseq model to a common interface between models and tasks.,FAIRSEQ features: i) a common interface across models and tasks that can be extended with user-supplied plug-ins; ii) efficient distributed and mixed precision training; enabling training over datasets with hundreds of millions of sentences on current hardware; iii) efficient distribution and mixed precision training.,FAIRSEQ features: i) a common interface across models and tasks that can be extended with user-supplied plug-ins; ii) efficient distributed and mixed precision training; enabling training over datasets with hundreds of millions of sentences on current hardware; iii) efficient distribution and mixed precision training.,The key idea of the paper is to extend the BaseFairsequence model to a shared interface between models and task.,FAIRSeq is distributed with a BSD license and is available on GitHub at https://www.pytorch.com/pytorches/fairseq/fairsequ.,FAIRSeq is distributed with a BSD license and is available on GitHub at https://www.pytorch.com/pytorches/fairseq/fairsequ.,The key idea in the paper is to use a learning rate scheduler to update the learning rate over the course of training.,"The paper also provides a new implementation of Adam, which is a memory efficient variant of Adam.",The paper also presents a new implementation for the inverse square root scheduler.,The paper also introduces a new learning rate scheduler for the inverse Square Root scheduler.,"The proposed method is evaluated on a variety of tasks, including translation, summarization, and language modeling.","Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling.","Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling.",This paper presents a new toolkit for a sequence modeling toolkit.,"In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production.","In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production.",The key idea is to extend the baseFairseq model to a common interface between models and tasks.,"FAIRSEQ features: i) a common interface across models and tasks that can be extended with user-supplied plug-ins; ii) efficient distributed and mixed precision training; enabling training over datasets with hundreds of millions of sentences on current hardware; iii) efficient distribution and mixed precision training, enabling training over data with hundreds of millions sentences on current hardware.",FAIRSEQ features: i) a common interface across models and tasks that can be extended with user-supplied plug-ins; ii) efficient distributed and mixed precision training; enabling training over datasets with hundreds of millions of sentences on current hardware; iii) efficient distribution and mixed precision training.,The key idea of the paper is to extend the BaseFairsequence model to a shared interface between models and task.,"We provide tasks for translation, language modeling, and classification.","We also abstract the methods through which the model interacts with the generation algorithm, e.g., beam search, through step-wise prediction.",The key idea in the paper is to use a learning rate scheduler to update the learning rate over the course of training.,We provide a variety of popular learning rates schedulers and implement them in PyTorches.,"We provide tasks for translation, language modeling, and classification.","The paper also provides a new implementation of Adam, which is a memory efficient variant of Adam.",We provide supervised learning rates for a variety of tasks.,We provide a comprehensive overview of the methods and provide a comprehensive list of experiments.,The paper also presents a new implementation for the inverse square root scheduler.,The paper also introduces a new learning rate scheduler for the inverse Square Root scheduler.,"The proposed method is evaluated on a variety of tasks, including translation, summarization, and language modeling."
We present a method for learning continuous word representations of words.,This paper presents a method for learning continuous word representations of words.,This paper proposes a new model for learning continuous representations of words.,"We use a skipgram model to learn word representations for character n-grams, and use subword information to learn word representation.","The method is based on a continuous skipgram model, which takes into account subword information.","The model is based on a log-based model, which is based on the n-gram information.","We evaluate our method on nine languages exhibiting different morphologies, showing the benefit of our approach.","The method is evaluated on nine languages exhibiting different morphologies, showing the benefit of our approach.","The model is trained on a large corpus of morphologically rich languages, and is evaluated on 9 languages exhibiting different morphologies.",We use n-gram count vectors to represent words.,"We propose a continuous skipgramm model, which takes in account subword information, and learns representations for character n-grams.","The model is evaluated on a number of languages, including French, Spanish, and Finnish.",We use the sum of n-gram vectors to represent word representations.,We use a morphologically rich language model to learn word representations.,We use morphological information to learn word embeddings.,We use the morphologically robust morphological information to learn word representation.,We use the n-gram counts vectors to represent character n- Grams.,"We present a method for learning continuous representations of words, based on a skipgram model.","This paper presents an extension of the continuous skipgram model (Mimicolov et al., 2013b) that takes into account subword information.",This paper proposes a new model for learning continuous representations of words.,"We use a n-gram skipgram model to learn representations of character n-grams, and use a subword information to learn representations for character n- Grams.","We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.","The model is based on a log-based model, which is based on the n-gram information.","We evaluate this method on nine languages exhibiting different morphologies, showing the benefit of our approach.","We propose a new representation for character n-grams, which takes into account sub word information.","The model is trained on a large corpus of morphologically rich languages, and is evaluated on 9 languages exhibiting different morphologies.","We use n-gram information to learn representation of character n Grams, and use this information to learn representation for character n Gram vectors.","We evaluate our model on nine languages with different morphologieses, showing the benefit.","The model is evaluated on a number of languages, including French, Spanish, and Finnish.","We use morphological information to learn representation from character n-Grads, and use the subword information to represent character n-Gazes.",We use a morphologically rich language model to represent words.,We use n-gram counts to represent words.
"We present GLUE, a benchmark of nine NLI tasks, a auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models.","This paper presents GLUE, a benchmark of nine NLI tasks, a auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models.","This paper presents a new benchmark for GLUE, a benchmark of 9 diverse NLU tasks, a auxiliary dataset for probing models for understanding specific linguistic phenomena.","The dataset includes a large variety of NLI tasks, including a wide range of domains, data quantities, and difficulties.","The dataset includes a large variety of NLI tasks, including a wide range of domains, data quantities, and difficulties.",The dataset is a supplementary dataset for evaluating and comparing models.,"We evaluate models that use ELMo, a powerful transfer learning technique, and state-of-the-art sentence representation models.","The dataset is designed to highlight common phenomena such as the use of world knowledge, logical operators, and lexical entailments, and basing our examples on naturally-occurring sentences from several domains.","The dataset is designed to highlight common phenomena such as the use of world knowledge, logical operators, and lexical entailments.","We evaluate models trained on a single task, using a BiL-STM between task-specific classifiers, and using six NLP researchers manually validate a random sample of the data.","We evaluate models that use ELMo, a powerful transfer learning technique, and state-of-the-art sentence representation models.","The dataset is built on top of the GLUE benchmark, which is a benchmark of nine diverse NLU tasks.","We evaluate models on a range of NLI tasks from a wide range, data quantities, difficulties, and domains.",We evaluate models trained on a single task and evaluate models that use the same architecture but are trained on a different task.,The dataset is an auxiliary dataset for evaluating models for understanding specific language phenomena.,"The dataset includes a large range of tasks, data quantities, and difficulties.",The dataset is used to evaluate the performance of a number of models on the GLUE dataset.,The dataset is also used to evaluate the best models on the BLUE dataset.,The authors also evaluate the best models for the GLUE task.,The dataset is created by a combination of a bi-label-sTM and a pretrained model on the task-specific classifier.,The authors also show that the proposed dataset is a good fit for the GLEU benchmark.,"We present GLUE, a benchmark of nine NLI tasks, a auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models.","This paper presents GLUE, a benchmark of nine NLI tasks, a auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models.","This paper presents a new benchmark for GLUE, a benchmark of 9 diverse NLU tasks, a auxiliary dataset for probing models for understanding specific linguistic phenomena.","The dataset includes a large variety of NLI tasks, including a wide range of domains, data quantities, and difficulties.","The dataset includes a large variety of NLI tasks, including a wide range of domains, data quantities, and difficulties.",The dataset is a supplementary dataset for evaluating and comparing models.,"We evaluate models that use ELMo, a powerful transfer learning technique, as well as state-of-the-art sentence representation models.","The dataset is designed to highlight common phenomena such as the use of world knowledge, logical operators, and lexical entailments, and basing our examples on naturally-occurring sentences from several domains.","The dataset is designed to highlight common phenomena such as the use of world knowledge, logical operators, and lexical entailments.","We evaluate models trained on a single task, using a BiL-STM between task-specific classifiers, and using six NLP researchers manually validate a random sample of the data.","We evaluate models that use ELMo, a powerful transfer learning technique, and state-of-the-art sentence representation models.","The dataset is built on top of the GLUE benchmark, which is a benchmark of nine diverse NLU tasks.","We evaluate models on a range of NLI tasks from a wide range, data quantities, difficulties, and domains.","We evaluate models trained on a single task, using a BiL-STM between task-specific classifiers.",The dataset is an auxiliary dataset for evaluating models for understanding specific language phenomena.,"The dataset includes a large range of tasks, data quantities, and difficulties.",The dataset is used to evaluate the performance of a number of models on the GLUE dataset.,The dataset is also used to evaluate the best models on the BLUE dataset.,The authors also evaluate the best models for the GLUE task.,The dataset is created by a combination of a bi-label-sTM and a pretrained model on the task-specific classifier.,The authors also show that the proposed dataset is a good fit for the GLEU benchmark.
This paper presents a controlled manual evaluation of machine translation performance.,This paper presents a controlled manual evaluation of machine translation performance.,"This paper proposes a new metric for machine translation, namely BLEU, which is a metric for machine Translation.",The controlled manual evaluation is based on the use of a metric based on the BLEU score.,"The controlled manual evaluation is based on the BLEU score, which is a metric that measures the quality of a machine translation model.","The authors propose a new python package, SACREBLEU, which automatically stores reference tokenization when sharing scores.",The controlled manually evaluation is based upon the use of metric-supplied reference tokenization.,The controlled manually evaluation is based upon the BLEU scores computed against different reference sets.,The authors show that the proposed method is a good way to measure the quality of a model.,The controlled hand evaluation is based in the sense that the controlled manual evaluation can be used to evaluate the performance of a machine translation model.,"The controlled hand evaluation is based in the sense that the controlled manual evaluations are not a single metric, but a constellation of parameterized methods.",We describe a controlled hand evaluation that is based on a metric-based metric based upon the BLEU scores.,"The controlled human evaluation is based primarily on the BLUE score, which is not a single parameterized method.",We show that the controlled hand evaluation can be used as a proxy for human evaluation of machine translation quality.,The controlled automatic evaluation is based solely on the BLE score.,"The controlled paper evaluation is based off of the BLEU scoring, which is based on a metric-supplied reference tokenization.",The controlled system is based on an LSTM-based approach.
